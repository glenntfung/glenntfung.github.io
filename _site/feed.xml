<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-06-18T23:46:23-05:00</updated><id>http://0.0.0.0:4000/feed.xml</id><entry><title type="html">Neural Networks, CNNs, RNNs, Transformers, and Beyond</title><link href="http://0.0.0.0:4000/blog/2025/05/20/nn/" rel="alternate" type="text/html" title="Neural Networks, CNNs, RNNs, Transformers, and Beyond" /><published>2025-05-20T19:00:00-05:00</published><updated>2025-05-20T19:00:00-05:00</updated><id>http://0.0.0.0:4000/blog/2025/05/20/nn</id><content type="html" xml:base="http://0.0.0.0:4000/blog/2025/05/20/nn/"><![CDATA[<p>This post is a very long introduction to various kinds of machine learning models and their ethical and societal implications. Jump to</p>

<ul>
  <li><a href="#neural-networks-the-foundation">Neural Networks: The Foundation</a>
    <ul>
      <li><a href="#training">Training</a></li>
    </ul>
  </li>
  <li><a href="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a>
    <ul>
      <li><a href="#variants-of-cnns">Variants of CNNs</a></li>
    </ul>
  </li>
  <li><a href="#fundamentals-of-language-processing">Fundamentals of Language Processing</a></li>
  <li><a href="#recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</a>
    <ul>
      <li><a href="#long-short-term-memory-lstms">Long Short-Term Memory (LSTMs)</a></li>
    </ul>
  </li>
  <li><a href="#transformers">Transformers</a>
    <ul>
      <li><a href="#different-types-of-transformers">Different Types of Transformers</a></li>
    </ul>
  </li>
  <li><a href="#generative-models">Generative Models</a>
    <ul>
      <li><a href="#generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</a></li>
      <li><a href="#variational-autoencoders-vaes">Variational Autoencoders (VAEs)</a></li>
      <li><a href="#diffusion-models">Diffusion Models</a></li>
    </ul>
  </li>
  <li><a href="#state-space-models-ssms">State Space Models (SSMs)</a></li>
  <li><a href="#graph-neural-networks-gnn">Graph Neural Networks (GNN)</a></li>
  <li><a href="#deep-reinforcement-learning">Deep Reinforcement Learning</a></li>
  <li><a href="#ethical-and-societal-implications">Ethical and Societal Implications</a></li>
</ul>

<h2 id="neural-networks-the-foundation">Neural Networks: The Foundation</h2>

<p>A neural network is a computational model inspired by the human brain. It consists of layers of interconnected neurons (or nodes), each performing a weighted summation followed by a non-linear activation function.</p>

<h3 id="mathematical-representation">Mathematical Representation</h3>

<p>Consider an input vector \(\mathbf{x}\in\mathbb{R}^n\), weights \(\mathbf{W}\in\mathbb{R}^{m\times n}\), biases \(\mathbf{b}\in\mathbb{R}^m\), and activation function \(f(\cdot)\). The output of a single layer is</p>

\[\mathbf{h} = f(\mathbf{W}\mathbf{x} + \mathbf{b}).\]

<p>Stacking these layers allows the network to learn hierarchical representations:</p>

\[\mathbf{h}^{(k+1)} = f(\mathbf{W}^{(k)} \mathbf{h}^{(k)} + \mathbf{b}^{(k)})\]

<p>The figure below illustrates this “stacking” process:</p>

<p><img src="/assets/pdf/nn/nn.png" alt="NN" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="training">Training</h3>

<p>It is now clear that training aims to obtain $\mathbf{W}$ and $\mathbf{b}$. Define a loss function, $L(\theta)$, to be minimized by adjusting $\mathbb{W}$ and $\mathbb{b}$, call them $\theta$. The optimizer achieves this by repeatedly calculating the gradient of the loss function, $\nabla_{\theta}L$, using backpropagation and updating the parameters via gradient descent. First, after a forward pass computes the network’s output and its resulting error or loss, the backpropagation algorithm performs a backward pass. During this pass, it efficiently calculates the gradient of the loss function, $\nabla_{\theta}L$, by applying the chain rule of calculus recursively from the final layer back to the first, determining how much each parameter contributed to the error. Finally, the gradient descent optimizer uses these calculated gradients to update all the network’s parameters, $\theta$, according to $\theta_{t+1} = \theta_t - \eta \nabla_{\theta}L(\theta_t)$, slightly adjusting them to minimize future error. To create sparsity, add a regularization penalty like the $\ell_1$ norm, $\lambda \sum |\theta_i|$, to the loss function, which actively forces many parameter values to become zero.</p>

<h3 id="theoretical-foundations">Theoretical Foundations</h3>

<p>The capacity of neural networks to approximate any continuous function on a compact domain is guaranteed by the universal approximation theorem(s), stating that a neural network with at least one hidden layer and a non-linear activation function can approximate any continuous function to an arbitrary degree of accuracy <a class="citation" href="#cybenko1989approximation">(Cybenko, 1989; Hornik et al., 1989)</a>. People later proved that a network with a fixed, minimal width can be a universal approximator, provided it can have arbitrary depth <a class="citation" href="#lu2017expressive">(Lu et al., 2017; Kidger &amp; Lyons, 2020)</a>. These theorems are purely existential, offering no guarantees on the efficiency of learning algorithms or the number of neurons required to achieve a prescribed approximation error.</p>

<p>Surprisingly, the loss surface of a large multilayer network is not fraught with poor local minima but is instead dominated by numerous <em>saddle points</em> and local minima whose loss values are qualitatively close to the global minimum <a class="citation" href="#choromanska2015loss">(Choromanska et al., 2015)</a>. Counterintuitively, overparameterization can improve generalization, a behavior reconciled by double descent, which subsumes the classical bias-variance trade‐off into a unified framework where increasing capacity beyond interpolation lowers test error due to implicit regularization by the optimization algorithm <a class="citation" href="#belkin2019reconciling">(Belkin et al., 2019)</a>.</p>

<h3 id="practical-implementation">Practical Implementation</h3>

<p>Modern deep learning hinges on flexible frameworks that abstract computational graphs, automatic differentiation, and hardware acceleration to streamline prototyping and large‐scale training. One important problem is the vanishing or exploding gradients problem, which arises in deep networks because backpropagation repeatedly multiplies gradients through each layer, causing the update signal to either shrink exponentially toward zero (vanishing), which prevents early layers from learning, or grow uncontrollably large (exploding), which makes the training process unstable. Initialization schemes like He initialization align weight variances with layer activations to mitigate this.</p>

<p>Batch normalization stabilizes activation distributions and accelerates deep convolutional training by explicitly normalizing layer inputs via batch‐wise mean and variance (enabling higher learning rates and less sensitivity to initialization) <a class="citation" href="#ioffe2015batch">(Ioffe &amp; Szegedy, 2015)</a>. Recently, researchers found that the dynamic tanh approach, which replaces all normalization layers in Transformers with a single learnable element-wise $\tanh(\alpha x)$, can further offer a practical, statistics-free alternative whenever batch or layer statistics are impractical or too costly <a class="citation" href="#zhu2025transformers">(Zhu et al., 2025)</a>.</p>

<p>Adaptive optimizers such as Adam adjust per‐parameter learning rates based on estimates of first and second moments, offering robustness in sparse or noisy gradient scenarios and often outperforming vanilla SGD in practice <a class="citation" href="#kingma2014adam">(Kingma, 2014)</a>, overcoming the saddle point problem. Effective hyperparameter tuning—through systematic searches, Bayesian methods, or bandit algorithms—and rigorous experiment tracking (e.g., with TensorBoard or Weights &amp; Biases) is critical for replicable state‐of‐the‐art results.</p>

<h3 id="empirical-understandings">Empirical Understandings</h3>

<p>Empirical scaling laws for language models demonstrate that cross‐entropy loss follows power‐law relationships with respect to model parameters, dataset size, and compute, allowing practitioners to predict performance gains and allocate resources optimally <a class="citation" href="#kaplan2020scaling">(Kaplan et al., 2020)</a>. The double descent phenomenon explains why enlarging model capacity can paradoxically reduce test error even after achieving zero training loss, unifying classical and modern generalization theories under one curve <a class="citation" href="#belkin2019reconciling">(Belkin et al., 2019)</a>. Moreover, the lottery ticket hypothesis reveals that within dense, randomly‐initialized networks lie sparse subnetworks (winning tickets) that, when trained in isolation, can match or exceed the accuracy of the full model, offering new directions for pruning and efficient inference <a class="citation" href="#frankle2018lottery">(Frankle &amp; Carbin, 2018)</a>.</p>

<h2 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h2>

<p>CNNs are specialized for data with spatial structure, like images. Instead of fully connected layers, they use convolutional layers to extract local patterns, such as edges in images (corresponding to shapes in the image). Multiple layers are involved in this process <a class="citation" href="#lecun2002gradient">(LeCun et al., 2002)</a>.</p>

<p><img src="/assets/pdf/nn/cnn.png" alt="CNN" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="mathematical-representation-1">Mathematical Representation</h3>

<p>A convolution operation involves a filter (or kernel) \(\mathbf{K} \in \mathbb{R}^{k \times k}\) sliding over the input \(\mathbf{X} \in \mathbb{R}^{n \times n}\):</p>

\[(\mathbf{X} * \mathbf{K})_{ij} = \sum_{p=0}^{k-1}\sum_{q=0}^{k-1} \mathbf{X}_{i+p, j+q} \mathbf{K}_{p, q}\]

<p>The output is called a <strong>feature map</strong>. The figure below illustrates this process:</p>

<p><img src="/assets/pdf/nn/conv.png" alt="Conv" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<p>The starting point of this illustration demonstrates that CNNs are naturally useful for images or videos — we can view each pixel as a single cell of the input above. Pooling layers (e.g., max pooling) then downsample these feature maps, reducing dimensionality:</p>

<p><img src="/assets/pdf/nn/pooling.png" alt="Pooling" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<p>The flattening process converts the feature maps in \(\mathbb{R}^{H \times W \times D}\) to a 1-D vector (e.g., in \(\mathbb{R}^{K}\)):</p>

<p><img src="/assets/pdf/nn/flat.png" alt="Flat" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<p>Lastly, a fully connected layer connects the vector to the output layer,</p>

\[\mathbf{h}^{(k+1)} = f(\mathbf{W}^{(k)} \mathbf{h}^{(k)} + \mathbf{b}^{(k)}).\]

<p>CNNs apply the same filter across the input and focus on local patches. Layers capture increasingly complex features (e.g., edges → textures → objects).</p>

<h3 id="foundations">Foundations</h3>

<p>The convolution operation embeds a translation equivariance prior by sharing the same kernel across spatial locations, drastically reducing the number of free parameters compared to fully connected layers and enabling the detection of local patterns regardless of their position in the image. Beyond parameter efficiency, the universal approximation properties of deep convolutional architectures stem from their ability to hierarchically compose simple linear filters and pointwise nonlinearities to approximate increasingly complex functions, a concept formalized in early work on multi-layer perceptrons and extended to convolutional settings <a class="citation" href="#lecun2002gradient">(LeCun et al., 2002)</a>. The scattering transform framework interprets CNNs as cascades of wavelet convolutions and modulus operations, proving Lipschitz stability to deformations—a proxy for robustness to small geometric perturbations—while still capturing discriminative signal variations (i.e., higher-order interactions) <a class="citation" href="#mallat2012group">(Mallat, 2012)</a>. Theoretical analyses have also shown a surprising result, where modern deep nets, including CNN, have enough capacity to memorize random labels (i.e., achieve zero training error on noise) with no explicit regularization <a class="citation" href="#zhang2016understanding">(Zhang et al., 2016)</a>.</p>

<p>Empirical studies of feature transferability reveal that early convolutional layers learn general patterns such as edge and texture detectors, while deeper layers capture task-specific semantics; transferring features from mid-level layers provides the best balance between generality and specificity for new tasks <a class="citation" href="#yosinski2014transferable">(Yosinski et al., 2014)</a>.</p>

<h3 id="variants-of-cnns">Variants of CNNs</h3>

<p>Early landmark models such as AlexNet <a class="citation" href="#krizhevsky2012imagenet">(Krizhevsky et al., 2012)</a> train on ImageNet <a class="citation" href="#deng2009imagenet">(Deng et al., 2009)</a> demonstrated that deep convolutional architectures trained on large-scale datasets with GPUs could achieve dramatic improvements in object recognition, introducing ReLU activations ($f(x)=\max(0,x)$), data augmentation techniques (in-memory reflections and intensity alternation), and dropout (temporarily randomly deactivating neurons) as a regularizer to mitigate co-adaptation of neurons. Subsequent architectures explored the impact of depth and filter granularity: VGGNets <a class="citation" href="#simonyan2014very">(Simonyan &amp; Zisserman, 2014)</a> showed that stacking small $3 \times 3$ convolutions to reach depths of 16–19 layers yields improved representational power and transferability across tasks, while Inception modules factorized convolutions into multiple filter sizes to better utilize computational resources and capture multi-scale context <a class="citation" href="#szegedy2015going">(Szegedy et al., 2015)</a>. The introduction of residual connections overcame optimization difficulties in very deep models by reformulating each layer as a residual mapping, enabling stable training of networks exceeding 100 layers and pushing error rates below 4% on ImageNet <a class="citation" href="#he2016deep">(He et al., 2016)</a>. More recently, compound scaling methods systematically balance depth, width, and resolution by a single coefficient, resulting in EfficientNet families that deliver superior accuracy-efficiency trade-offs and generalize effectively across transfer-learning benchmarks <a class="citation" href="#tan2019efficientnet">(Tan &amp; Le, 2019; Tan &amp; Le, 2021)</a>.</p>

<h2 id="fundamentals-of-language-processing">Fundamentals of Language Processing</h2>

<p>Before introducing sequential models, let’s first see how we can represent texts numerically, which is not as straightforward as image representation (pixel grids). Remember <strong>models understand numbers</strong>.</p>

<p>Before embedding, raw text is segmented into subword tokens via algorithms such as Byte-Pair Encoding, which greedily merges the most frequent character pairs to yield a fixed-size vocabulary that balances morphological expressiveness and open-vocabulary coverage <a class="citation" href="#sennrich2015neural">(Sennrich et al., 2015)</a>.</p>

<blockquote>
  <p><strong>Byte-Pair Encoding (BPE) Example</strong></p>

  <ol>
    <li>
      <p><strong>Initial State:</strong> Start with a corpus (e.g., <code class="language-plaintext highlighter-rouge">low lower lowest</code>) and break words into characters plus an end-of-word marker (<code class="language-plaintext highlighter-rouge">l o w &lt;/w&gt;</code>). The initial vocabulary is just these characters.</p>
    </li>
    <li><strong>Greedy Merging:</strong> Iteratively find the most frequent adjacent pair and merge it into a new token.
      <ul>
        <li><strong>Step 1:</strong> The pair <code class="language-plaintext highlighter-rouge">l o</code> is most frequent. Merge it to create the token <code class="language-plaintext highlighter-rouge">lo</code>. The vocabulary is now <code class="language-plaintext highlighter-rouge">[l, o, w, ..., lo]</code>.</li>
        <li><strong>Step 2:</strong> The pair <code class="language-plaintext highlighter-rouge">lo w</code> becomes the most frequent. Merge it to create <code class="language-plaintext highlighter-rouge">low</code>. The vocabulary is now <code class="language-plaintext highlighter-rouge">[l, o, w, ..., lo, low]</code>.</li>
      </ul>
    </li>
    <li><strong>Result:</strong> This continues until a target vocabulary size is reached.
      <ul>
        <li><strong>Morphological Expressiveness:</strong> A known word like <code class="language-plaintext highlighter-rouge">lowest</code> is tokenized into meaningful parts, like <code class="language-plaintext highlighter-rouge">[low, est]</code>.</li>
        <li><strong>Open-Vocabulary Coverage:</strong> An unknown word like <code class="language-plaintext highlighter-rouge">slower</code> can still be represented by falling back to known subwords and characters, like <code class="language-plaintext highlighter-rouge">[s, l, o, w, er]</code>.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Each token $t_i$ is then represented as a one-hot vector $x_i\in\mathbb{R}^{|V|}$, where $|V|$ is the vocabulary size, and mapped into a dense embedding $e_i = E^\top x_i$ using an embedding matrix $E\in\mathbb{R}^{|V|\times d}$, capturing lexical semantics in a continuous space <a class="citation" href="#mikolov2013efficient">(Mikolov et al., 2013)</a>. Subword tokenization is the most efficient and manageable tokenization method thus far, as opposed to character and word tokenizations. Transformers inject positional information lost by parallel processing, a fixed sinusoidal encoding $P\in\mathbb{R}^{n\times d}$ is added, where</p>

\[P_{i,2k} = \sin\!\bigl(i/10000^{2k/d}\bigr),\quad
P_{i,2k+1} = \cos\!\bigl(i/10000^{2k/d}\bigr),\]

<p>yielding $Z = [e_1; \dots; e_n] + P$ as the input to subsequent layers. The famous attention mechanism then comes into play <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p>

<p>For ideographic languages such as Chinese and Japanese, the default approach treats each character as a base token, but recent sub-character methods <a class="citation" href="#si2023sub">(Si et al., 2023; Nguyen et al., 2017)</a> first transliterate characters into sequences of glyph strokes or phonetic radicals before applying BPE, allowing models to inject rich visual and pronunciation.</p>

<h2 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h2>

<p>RNNs <a class="citation" href="#elman1990finding">(Elman, 1990)</a> excel at processing sequential data, such as time series or text. They maintain a memory of previous inputs via a hidden state, allowing them to model temporal dependencies.</p>

<h3 id="mathematical-representation-2">Mathematical Representation</h3>

<p>RNNs process data sequentially. At each time step \(t\), an input \(\mathbf{x}_t\) is provided. The sequence of inputs can be represented as \(\mathbf{x}_1\), \(\mathbf{x}_2\), up to \(\mathbf{x}_T\), where \(T\) is the total number of time steps. RNNs maintain a hidden state, \(\mathbf{h}_t\), acting as the network’s memory, updated at each time step based on the current input and the previous hidden state.</p>

<p>Given input \(\mathbf{x}_t\), hidden state \(\mathbf{h}_t\), and weights \(\mathbf{W}_{xh}\), \(\mathbf{W}_{hh}\), the hidden state is updated as</p>

\[\mathbf{h}_t = f(\mathbf{W}_{xh} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{b}).\]

<p>The output \(\mathbf{y}_t\) is computed as:</p>

\[\mathbf{y}_t = g(\mathbf{W}_{hy} \mathbf{h}_t + \mathbf{c}).\]

<p><img src="/assets/pdf/nn/rnn.png" alt="RNN" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="limitations-and-development">Limitations and Development</h3>

<p>RNNs suffer inherently from gradients that either vanish or explode exponentially with depth in time when signals are propagated over many time steps, making naive RNNs impractical for long‐term dependencies <a class="citation" href="#hochreiter1998vanishing">(Hochreiter, 1998; Bengio et al., 1994; Pascanu et al., 2013)</a>. Long Short-Term Memory cells <a class="citation" href="#hochreiter1997long">(Hochreiter &amp; Schmidhuber, 1997)</a> mitigate this by embedding gating units that learn to preserve information in a constant‐error carousel, thereby enabling the modeling of arbitrarily distant dependencies without gradient collapse <a class="citation" href="#hochreiter1997long">(Hochreiter &amp; Schmidhuber, 1997)</a>. Subsequent work on Recurrent Highway Networks <a class="citation" href="#zilly2017recurrent">(Zilly et al., 2017)</a> extended depth within each time step, applying gated residual connections to achieve deep transition functions that retain the LSTM’s long‐range memory while improving representational capacity. Alternative approaches constrain the recurrent weight matrix to be orthogonal or unitary, ensuring gradient norms remain constant and thus preserving signal propagation over arbitrary horizons without numerical instability <a class="citation" href="#mhammedi2017efficient">(Mhammedi et al., 2017; Arjovsky et al., 2016)</a>. Further, continuous‐time formulations like Neural ODEs reimagine recurrence as the discretization of a differential equation, offering a unified view of depth and time and opening the door to adaptive computation and memory allocation strategies <a class="citation" href="#chen2018neural">(Chen et al., 2018)</a>.</p>

<h3 id="long-short-term-memory-lstms">Long Short-Term Memory (LSTMs)</h3>

<p>LSTMs <a class="citation" href="#hochreiter1997long">(Hochreiter &amp; Schmidhuber, 1997)</a> improve on standard RNNs by controlling the flow of information, using gates to selectively remember, forget, or output information, allowing the network to retain long-term dependencies. The graph below illustrates this sequential structure, which is structurally similar to RNNs:</p>

<p><img src="/assets/pdf/nn/lstm.png" alt="LSTM" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<h4 id="mathematical-representation-3">Mathematical Representation</h4>

<p>Here, \(\sigma(\cdot)\) is the sigmoid activation, \(\tanh(\cdot)\) is the hyperbolic tangent, and \(\odot\) represents element-wise multiplication. At each time step \(t\), the LSTM maintains long-term memory called a cell state (\(\mathbf{C}_t\)) and short-term memory called a hidden state (\(\mathbf{h}_t\)). They also introduce 3 types of gates:</p>

<ol>
  <li>
    <p><strong>Forget Gate (\(\mathbf{f}_t\))</strong>:
Decides which information to discard from the cell state:</p>

\[\mathbf{f}_t = \sigma(\mathbf{W}_f \mathbf{x}_t + \mathbf{U}_f \mathbf{h}_{t-1} + \mathbf{b}_f).\]
  </li>
  <li>
    <p><strong>Input Gate (\(\mathbf{i}_t\))</strong>:
Decides which new information to add to the cell state:</p>

\[\mathbf{i}_t = \sigma(\mathbf{W}_i \mathbf{x}_t + \mathbf{U}_i \mathbf{h}_{t-1} + \mathbf{b}_i).\]

    <p><strong>Candidate Cell State</strong>:
The candidate cell state (\(\tilde{\mathbf{C}}_t\)) is computed as:</p>

\[\tilde{\mathbf{C}}_t = \tanh(\mathbf{W}_C \mathbf{x}_t + \mathbf{U}_C \mathbf{h}_{t-1} + \mathbf{b}_C).\]
  </li>
  <li>
    <p><strong>Output Gate (\(\mathbf{o}_t\))</strong>:
Decides the output based on the hidden state and cell state:</p>

\[\mathbf{o}_t = \sigma(\mathbf{W}_o \mathbf{x}_t + \mathbf{U}_o \mathbf{h}_{t-1} + \mathbf{b}_o)\]
  </li>
</ol>

<h5 id="updating-the-states">Updating the States</h5>

<ol>
  <li>
    <p><strong>Cell State Update</strong>:</p>

\[\mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t\]

    <p>The forget gate decides what to discard, and the input gate decides what to add.</p>
  </li>
  <li>
    <p><strong>Hidden State Update</strong>:</p>

\[\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{C}_t)\]
  </li>
</ol>

<p>The graph below illustrates this updating process:</p>

<p><img src="/assets/pdf/nn/lstmnode.png" alt="LSTM node" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="practical-implementation-1">Practical Implementation</h3>

<p>Training recurrent models requires balancing sequence length, computational budget, and numerical stability; truncated backpropagation through time (TBPTT) <a class="citation" href="#williams1989learning">(Williams &amp; Zipser, 1989)</a> limits gradient propagation to manageable windows while approximating full‐sequence gradients, and gradient clipping <a class="citation" href="#pascanu2013difficulty">(Pascanu et al., 2013)</a> prevents rare but disastrous exploding updates. Layer and weight regularization techniques—such as DropConnect in the AWD‐LSTM architecture <a class="citation" href="#merity2017regularizing">(Merity et al., 2017)</a> and variational dropout <a class="citation" href="#kingma2015variational">(Kingma et al., 2015)</a>—act directly on recurrent weights and activations to reduce overfitting in language modeling tasks, allowing smaller datasets to yield robust sequence predictors. Modern deep learning frameworks provide native implementations of these gating and optimization schemes, and tools like mixed‐precision training <a class="citation" href="#micikevicius2017mixed">(Micikevicius et al., 2017)</a> and distributed sequence parallelism <a class="citation" href="#merity2017regularizing">(Merity et al., 2017; Korthikanti et al., 2023; Jacobs et al., 2023)</a> make it feasible to train very deep or very long‐sequence models on GPUs and TPUs with reproducible results <a class="citation" href="#merity2017regularizing">(Merity et al., 2017)</a>.</p>

<h3 id="empirical-understandings-1">Empirical Understandings</h3>

<p>Empirical benchmarks on language modeling datasets reveal that carefully regularized LSTMs such as AWD‐LSTM <a class="citation" href="#merity2017regularizing">(Merity et al., 2017)</a> achieve state‐of‐the‐art perplexities on Penn Treebank <a class="citation" href="#marcus1993building">(Marcus et al., 1993)</a> and WikiText‐2 <a class="citation" href="#merity2016pointer">(Merity et al., 2016)</a>, demonstrating the continued relevance of gated recurrence for moderate‐scale tasks. However, scaling studies show that beyond a certain compute and data threshold, self‐attention architectures outperform traditional RNNs in both speed and quality <a class="citation" href="#kaplan2020scaling">(Kaplan et al., 2020)</a>, prompting hybrid approaches that inject attention mechanisms into LSTM backbones <a class="citation" href="#bahdanau2014neural">(Bahdanau et al., 2014)</a> or employ Neural ODE layers for continuous modeling <a class="citation" href="#chen2018neural">(Chen et al., 2018)</a>. Ablation experiments on gating variants and transition depths indicate that deeper recurrent transitions and highway connections yield diminishing returns beyond a handful of layers per step, suggesting that future gains will depend on novel memory‐access patterns or adaptive computation time mechanisms <a class="citation" href="#desbouvries2023expressivity">(Desbouvries et al., 2023; Zilly et al., 2017)</a>. As attention‐first models (introduced next section) continue to dominate, the most promising directions revive recurrence through continuous dynamics, orthogonal memory networks, and differentiable neural computers that combine the best of gating, memory, and attention in a unified framework.</p>

<h2 id="transformers">Transformers</h2>

<p>In the transformer <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>, we represent an input sequence of $n$ tokens by their embeddings $X\in\mathbb{R}^{n\times d}$ and compute three projections—queries $Q=XW^Q$, keys $K=XW^K$, and values $V=XW^V$—each in $\mathbb{R}^{n\times d_k}$.  Self‐attention is then given by</p>

\[\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\bigl(QK^{\!\top}/\sqrt{d_k}\bigr)\,V\,.\]

<p><img src="/assets/pdf/nn/att.png" alt="Scaled Dot-Product Attention" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<p>Multi‐head attention runs this in parallel for $h$ heads and concatenates the results:</p>

\[\mathrm{MultiHead}(X)=\mathrm{Concat}\bigl(\mathrm{head}_1,\dots,\mathrm{head}_h\bigr)\,W^O,\quad
\mathrm{head}_i=\mathrm{Attention}(XW^Q_i,XW^K_i,XW^V_i).\]

<p><img src="/assets/pdf/nn/mh.png" alt="Multi‐head attention" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<p>Each transformer layer applies a residual connection plus layer normalization, $\tilde X=\mathrm{LayerNorm}\bigl(X+\mathrm{MultiHead}(X)\bigr)$, followed by a position‐wise feed‐forward network</p>

\[\mathrm{FFN}(\tilde X)=\sigma(\tilde XW_1+b_1)\,W_2+b_2\]

<p>and another residual‐norm step $\mathrm{LayerNorm}\bigl(\tilde X+\mathrm{FFN}(\tilde X)\bigr)$.  Stacking $L$ such layers yields the final contextual representations used for downstream prediction.</p>

<p><img src="/assets/pdf/nn/1t.png" alt="Single Transformer Layer" style="max-width:90%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="different-types-of-transformers">Different Types of Transformers</h3>

<p>Beyond the canonical vanilla Transformer, nearly every variant introduces one or more mathematical tweaks to attention, embeddings, or the layer‐stacking strategy.  A common class of modifications concerns positional information, via learned absolute embeddings (BERT <a class="citation" href="#devlin2019bert">(Devlin et al., 2019)</a>, GPT-2/3 <a class="citation" href="#radford2019language">(Radford et al., 2019; Brown et al., 2020)</a>, RoBERTa <a class="citation" href="#liu2019roberta">(Liu et al., 2019)</a>), relative position biases (Transformer-XL <a class="citation" href="#dai2019transformer">(Dai et al., 2019)</a>, T5 <a class="citation" href="#raffel2020exploring">(Raffel et al., 2020)</a>, DeBERTa <a class="citation" href="#he2020deberta">(He et al., 2020)</a>), or rotary position embeddings (RoFormer <a class="citation" href="#su2024roformer">(Su et al., 2024)</a>, GPT-NeoX <a class="citation" href="#black2022gpt">(Black et al., 2022)</a>). In the original model, we add fixed sinusoidal encodings $P\in\mathbb{R}^{n\times d}$ so that the input to layer 1 is $X+P$. Later work replaces these with learned embeddings $P_\theta$, <em>relative</em> position biases $B_{ij}$, so that attention becomes</p>

\[\mathrm{softmax}\bigl((QK^\top + B)/\sqrt{d_k}\bigr)\,V,\]

<p>where $B\in\mathbb{R}^{n\times n}$ depends only on $i-j$, or even <em>rotary</em> embeddings that apply a learnable $2 \times 2$ rotation to each pair of dimensions in $Q$ and $K$ before dot‐product.  Such tweaks allow the model to better generalize to sequences longer than it saw in training, or to bias attention toward nearby tokens without explicit masking.</p>

<p>Another rich vein of innovation is efficient or specialized attention.  For truly long sequences, full $n\times n$ attention is quadratic in cost; sparse‐attention variants insert a structured mask $M$ so</p>

\[\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\bigl((QK^\top + M)/\sqrt{d_k}\bigr)V,\]

<p>where $M_{ij}=-\infty$ for disallowed pairs (e.g. sliding windows in Longformer <a class="citation" href="#beltagy2020longformer">(Beltagy et al., 2020)</a> or random/global tokens in BigBird <a class="citation" href="#zaheer2020big">(Zaheer et al., 2020)</a>).  Low‐rank or kernelized methods approximate</p>

\[\mathrm{softmax}(QK^\top)\approx \phi(Q)\,\phi(K)^\top\]

<p>via feature maps $\phi$, yielding linear time (Performer <a class="citation" href="#choromanski2020rethinking">(Choromanski et al., 2020)</a>).  Other approaches project keys and values to a lower dimension: Linformer <a class="citation" href="#wang2020linformer">(Wang et al., 2020)</a> posits learnable $E\in\mathbb{R}^{n\times k}$, $F\in\mathbb{R}^{n\times k}$ so that $K’=E^\top K$, $V’=F^\top V$, reducing attention to $\mathrm{softmax}(QK’^\top)V’$.  Finally, mixtures‐of‐experts (Switch Transformers <a class="citation" href="#fedus2022switch">(Fedus et al., 2022)</a>) replace each feed‐forward block with a routing mechanism $G(x)$ that selects among $m$ experts, so</p>

\[\mathrm{MoE}(x) = \sum_{e=1}^m G_e(x)\bigl(W_2^{(e)}\,\sigma(W_1^{(e)}x)\bigr),\]

<p>trading depth for conditional computation. Other architectures include GShard <a class="citation" href="#lepikhin2020gshard">(Lepikhin et al., 2020)</a> and GLaM <a class="citation" href="#du2022glam">(Du et al., 2022)</a>.</p>

<p>Together, these mathematical tweaks—positional biases, sparse or low‐rank attention, kernel approximations, adaptive-depth recurrence (Universal Transformer <a class="citation" href="#dehghani2018universal">(Dehghani et al., 2018)</a>), and conditional computation—form a rich taxonomy under the transformer umbrella, each tailored to specific tasks, modalities, or resource constraints.</p>

<h3 id="theoretical-understandings">Theoretical Understandings</h3>

<p>A Transformer layer computes scaled dot-product attention, where queries, keys, and values are linear projections of the same input; the resulting attention matrix is then normalized by $\sqrt{d_k}$ to maintain gradient stability, and softmaxed to produce a distribution over positions <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>. Multi-head attention extends this by learning multiple sets of projections, allowing the model to jointly attend to information from different representation subspaces at distinct positions, which empirically enhances expressivity and enables parallel processing of dependencies. Formal analysis reveals that self-attention matrices can approximate arbitrary sparse matrices—thus capturing selective interactions among tokens—provided sufficient hidden dimensionality, granting Transformers a universal approximation property for sequence-to-sequence functions <a class="citation" href="#likhosherstov2021expressive">(Likhosherstov et al., 2021)</a>. Positional encodings—either fixed sinusoidal functions or learned embeddings—inject order information lost by the permutation-invariant attention mechanism, allowing the network to distinguish between positions in a sequence while preserving the ability to generalize to longer sequences than seen during training. Recent theoretical work also explores linearized and sparse variants that reduce the quadratic complexity of full attention to linear or near-linear bounds, trading off exactness for scalability without sacrificing universal expressivity in the limit <a class="citation" href="#child2019generating">(Child et al., 2019; Choromanski et al., 2020)</a>.</p>

<h3 id="practical-implementation-2">Practical Implementation</h3>

<p>Effective Transformer training hinges on stabilized optimization. The AdamW <a class="citation" href="#loshchilov2017decoupled">(Loshchilov &amp; Hutter, 2017)</a> optimizer decouples weight decay from gradient updates, mitigating the tendency of adaptive methods to over-regularize while preserving the fast convergence of Adam; coupled with a linear warmup schedule for the learning rate (often over the first 10% of training steps), it prevents instability caused by large initial updates <a class="citation" href="#kosson2024analyzing">(Kosson et al., 2024)</a>. Gradient clipping <a class="citation" href="#pascanu2013difficulty">(Pascanu et al., 2013)</a> is commonly employed to bound the norm of gradients, curtailing occasional spikes during backpropagation that could derail learning, especially in deep or high-capacity models. Frameworks such as the <a href="https://huggingface.co/docs/transformers/en/index">Hugging Face Transformers library</a> provide modular building blocks—pretrained checkpoints, tokenizer classes, and optimized training loops—enabling researchers and practitioners to experiment with architectures like BERT, GPT, T5, and beyond using both <a href="https://pytorch.org/">PyTorch</a> and <a href="https://www.tensorflow.org/">TensorFlow</a> backends with minimal boilerplate. Mixed-precision training (via NVIDIA’s Apex or native AMP) significantly reduces memory usage and increases throughput by storing activations and performing many computations in a 16-bit floating-point format; to maintain numerical stability, a 32-bit master copy of the weights is used for accumulating gradients, a process that necessitates dynamic loss scaling to prevent underflow of small gradient values. Recent adapter-based fine-tuning methods such as LoRA <a class="citation" href="#hu2022lora">(Hu et al., 2022)</a> inject low-rank parameter updates into attention layers, slashing the number of trainable parameters for efficient domain adaptation without full-model retraining. Additionally, in-context learning allows large-scale Transformers to perform novel tasks by conditioning solely on a handful of demonstration examples in the input prompt, without any gradient updates to model parameters, a meta-learning capability that emerges only at sufficient model scale and is predictive of downstream few-shot performance <a class="citation" href="#brown2020language">(Brown et al., 2020)</a>.</p>

<h3 id="empirical-understandings-2">Empirical Understandings</h3>

<p>Empirical scaling laws for Transformers reveal that cross-entropy loss on language tasks follows a power-law decay as a function of model parameters, dataset size, and compute budget, enabling precise forecasts of performance improvements for scale investments <a class="citation" href="#kaplan2020scaling">(Kaplan et al., 2020)</a>. At extreme scales, emergent capabilities—such as few-shot in-context learning <a class="citation" href="#brown2020language">(Brown et al., 2020)</a>, chain-of-thought reasoning <a class="citation" href="#wei2022chain">(Wei et al., 2022)</a>, and compositional generalization <a class="citation" href="#anil2022exploring">(Anil et al., 2022)</a>—materialize abruptly and unpredictably, indicating qualitative shifts in model behavior that defy simple extrapolation from smaller models <a class="citation" href="#wei2022emergent">(Wei et al., 2022)</a>. Benchmarks comparing encoder-only models (e.g., BERT), decoder-only models (e.g., GPT), and encoder-decoder models (e.g., T5) demonstrate trade-offs between understanding and generation: encoder-only excels on classification and extraction tasks, decoder-only leads on open-ended generation, and encoder-decoder offers strong performance in sequence transduction. Fine-tuning studies show that the highest layers capture task-specific features while mid-layers encode transferable linguistic abstractions, guiding strategies for parameter freezing or adapter insertion during domain adaptation <a class="citation" href="#jurafsky2025speech">(Jurafsky &amp; Martin, 2025)</a>. As attention-driven models continue to dominate, the frontier now lies in integrating external memory <a class="citation" href="#graves2014neural">(Graves et al., 2014)</a>, adaptive computation time, and hybrid architectures that marry recurrence, attention, and continuous-depth dynamics to push the envelope of sequence modeling further <a class="citation" href="#israel2025enabling">(Israel et al., 2025)</a>.</p>

<h2 id="generative-models">Generative Models</h2>

<p>Generative modeling is one very important field in current applications and it encompasses a variety of approaches that trade off sample quality, training stability, and computational cost.</p>

<h3 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h3>

<p>A GAN consists of two neural networks—$G$ generates candidate samples from random noise, and $D$ discriminates between real and generated data—trained in a minimax game that converges when $G$ reproduces the true data distribution and $D$ cannot distinguish samples <a class="citation" href="#goodfellow2014generative">(Goodfellow et al., 2014)</a>. The core objective</p>

\[\min_G \max_D \;V(D,G)
=\mathbb{E}_{x\sim p_{\text{data}}}[\log D(x)]
+\mathbb{E}_{z\sim p_z}[\log(1 - D(G(z)))]\]

<p>encodes a zero-sum game where $D$ seeks to maximize its classification accuracy while $G$ seeks to fool $D$. Intuitively, this adversarial setup avoids explicitly defining a distance metric between distributions; instead, $D$ implicitly shapes $G$’s loss.</p>

<p>GANs are favored when high-resolution, perceptually realistic samples are required—image synthesis, style transfer (e.g., pix2pix <a class="citation" href="#isola2017image">(Isola et al., 2017)</a>), and data augmentation in medical imaging. However, training dynamics can oscillate or diverge, and GANs commonly suffer from mode collapse, where $G$ outputs limited variations, undermining data diversity <a class="citation" href="#kossale2022mode">(Kossale et al., 2022)</a>. Techniques like Wasserstein GANs <a class="citation" href="#arjovsky2017wasserstein">(Arjovsky et al., 2017)</a>, two-time-scale updates <a class="citation" href="#heusel2017gans">(Heusel et al., 2017)</a>, and minibatch discrimination <a class="citation" href="#salimans2016improved">(Salimans et al., 2016)</a> partially mitigate these failures, but stable convergence remains a challenge.</p>

<h3 id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)</h3>

<p>VAEs frame generative modeling as approximate inference in a probabilistic graphical model <a class="citation" href="#kingma2013auto">(Kingma et al., 2013)</a>. An encoder network parameterizes a variational posterior $q_\phi(z\mid x)$ over latent $z$, and a decoder network defines $p_\theta(x\mid z)$. Training maximizes the evidence lower bound,</p>

\[\mathcal{L}_{\theta,\phi}(x)
=\mathbb{E}_{z\sim q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
-\mathrm{KL}\bigl(q_\phi(z\mid x)||p(z)\bigr),\]

<p>balancing reconstruction fidelity against a Kullback–Leibler penalty that regularizes $q_\phi$ toward the prior $p(z)=\mathcal{N}(0,I)$. The reparameterization trick—expressing $z=E_\phi(x)+\sigma_\phi(x)!\odot!\epsilon$ with $\epsilon\sim\mathcal{N}(0,I)$—enables gradient descent.</p>

<p>VAEs excel in learning smooth, disentangled latent spaces for downstream tasks like interpolation, anomaly detection, and semi-supervised classification. They train reliably via maximum likelihood principles but often yield blurry outputs due to pixel-wise losses and can collapse the posterior to the prior (posterior collapse), losing latent expressivity <a class="citation" href="#wang2021posterior">(Wang et al., 2021)</a>. Intuitively, one can imagine this as a librarian giving up on complex filing systems and simply dumping every single book, regardless of what it is, directly into the central pile. Remedies include KL annealing <a class="citation" href="#bowman2015generating">(Bowman et al., 2015; Fu et al., 2019)</a>, $\beta$-VAEs <a class="citation" href="#higgins2017beta">(Higgins et al., 2017)</a>, and alternative divergences <a class="citation" href="#davidson2018hyperspherical">(Davidson et al., 2018)</a>.</p>

<h3 id="diffusion-models">Diffusion Models</h3>

<p>Diffusion models cast generation as the learned reversal of a gradual, data-corrupting noising process <a class="citation" href="#sohl2015deep">(Sohl-Dickstein et al., 2015; Ho et al., 2020)</a>. This forward process is defined as a Markov chain that systematically adds Gaussian noise to the data over successive steps:</p>

\[q(x_t\mid x_{t-1})=\mathcal{N}\bigl(x_t;\sqrt{1-\beta_t}\,x_{t-1},\;\beta_tI\bigr)\]

<p>To generate new samples, a neural network—often a U-Net—is trained to learn the reverse “denoising” process, $p_\theta(x_{t-1}\mid x_t)$. The network is optimized by minimizing a variational upper bound on the negative log-likelihood, which trains it to reconstruct the data by incrementally removing the noise step by step.</p>

<p>By sidestepping adversarial objectives, diffusion models offer stable training and have achieved superior fidelity in image and audio synthesis—powering DALL-E 2 <a class="citation" href="#ramesh2022hierarchical">(Ramesh et al., 2022)</a>, and Stable Diffusion <a class="citation" href="#rombach2022high">(Rombach et al., 2022)</a>—while supporting inpainting, super-resolution, and conditioned generation. Their main limitation is inference cost: thousands of sequential denoising steps lead to slow sampling and high compute demands, motivating research on accelerated samplers and trading off steps for quality.</p>

<h2 id="state-space-models-ssms">State Space Models (SSMs)</h2>

<p>State space models formalize sequential data by positing an unobserved (latent) state $x_t\in\mathbb{R}^n$ that evolves linearly under additive Gaussian noise and generates observations $y_t\in\mathbb{R}^m$ through another linear mapping. In the canonical form,</p>

\[x_t = A\,x_{t-1} + B\,u_t + w_t,\quad w_t\sim\mathcal N(0,Q),\quad
y_t = C\,x_t + D\,u_t + v_t,\quad v_t\sim\mathcal N(0,R),\]

<p>where $u_t$ denotes known inputs, and $Q,R$ are covariance matrices governing process and measurement noise <a class="citation" href="#kalman1960new">(Kalman, 1960; Kalman, 1963; Durbin &amp; Koopman, 2012)</a>.</p>

<p>Intuitively, the latent state $x_t$ captures the system’s memory and structure, while Bayesian filtering algorithms (e.g., the Kalman filter) recursively update the posterior $\mathbb P(x_t\mid y_{1:t})$ as new data arrive. The Kalman filter computes the minimum-variance estimate of the latent state via a predict–update cycle:</p>

\[\hat x_{t|t-1}=A\hat x_{t-1|t-1}+B u_t,\quad P_{t|t-1}=A P_{t-1|t-1}A^\top+Q,\]

\[K_t=P_{t|t-1}C^\top\bigl(CP_{t|t-1}C^\top+R\bigr)^{-1},\quad
\hat x_{t|t}=\hat x_{t|t-1}+K_t\,(y_t-C\hat x_{t|t-1}),\quad
P_{t|t}=(I-K_tC)\,P_{t|t-1}.\]

<p>It yields the optimal minimum mean-square error estimate by minimizing $\mathbb{E}||x_t-\hat{x}_{t\mid t}||^2$ under Gaussian noise assumptions. When exact Gaussian updates become intractable—due to nonlinearity or high dimensionality—sequential Monte Carlo and MCMC methods provide flexible approximate inference.</p>

<p>These models are prized for handling noisy, partially observed time series: they naturally accommodate measurement error, cope with missing data, and decompose signals into interpretable components such as trend and seasonality. The linear Gaussian assumption, however, can fail when dynamics are strongly nonlinear or noise is non-Gaussian. The extended Kalman filter may diverge under severe nonlinearity, and even unscented variants can underperform if noise covariances are misspecified. More critically, simple SSMs can exhibit biased or imprecise parameter estimates when measurement error dominates true signal variance <a class="citation" href="#julier2004unscented">(Julier &amp; Uhlmann, 2004; Auger-Méthé et al., 2016)</a>.</p>

<p>In practice, state space models underpin econometric forecasting and structural time series analysis <a class="citation" href="#harvey1990forecasting">(Harvey, 1990)</a>, speech recognition via continuous-emission HMMs <a class="citation" href="#rabiner2002tutorial">(Rabiner, 2002)</a>, robotic localization and SLAM through probabilistic state estimation, and modern deep latent-variable learning such as deep Kalman filters for counterfactual inference in health care and vision <a class="citation" href="#krishnan2015deep">(Krishnan et al., 2015)</a>. Recent advances in Gaussian process state-space models and fully variational inference further extend classical SSMs to nonparametric, high-dimensional settings <a class="citation" href="#fan2023free">(Fan et al., 2023; Särkkä &amp; Svensson, 2023)</a>.</p>

<h2 id="graph-neural-networks-gnns">Graph Neural Networks (GNNs)</h2>

<p>GNNs generalize deep neural networks to graph-structured data by iteratively aggregating information from each node’s local neighbourhood. Formally, a $k$-th layer representation of node $v$ is given by</p>

\[h_v^{(k)} = \text{update}^{(k)}\bigl(h_v^{(k-1)},\;\text{aggregate}^{(k)}\{\,(h_u^{(k-1)},e_{uv}):u\in\mathcal{N}(v)\}\bigr),\]

<p>where $h_v^{(k)}\in\mathbb{R}^d$ and $e_{uv}$ denotes edge features <a class="citation" href="#gori2005new">(Gori et al., 2005)</a>. This message-passing paradigm casts learning as finding a fixed point of a contraction mapping over node states, ensuring convergence under mild conditions <a class="citation" href="#scarselli2008graph">(Scarselli et al., 2008)</a>. A widely used special case is the graph convolutional network, which approximates localized spectral graph convolutions via</p>

\[\mathbf{H} = \sigma\bigl(\tilde{\mathbf{D}}^{-\frac12}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac12}\mathbf{X}\mathbf{\Theta}\bigr),\]

<p>with $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$ and $\tilde{\mathbf{D}}$ the augmented degree matrix, yielding scalable filters on irregular domains <a class="citation" href="#kipf2016semi">(Kipf &amp; Welling, 2016)</a>.</p>

<p>Intuitively, GNNs capture both attribute and structural information by smoothing and propagating features along edges, effectively exploiting the homophily principle prevalent in many real-world networks; empirical evidence shows such pooling enhances node and graph embeddings for downstream tasks <a class="citation" href="#wu2020comprehensive">(Wu et al., 2020)</a>. However, the representational capacity of standard GNNs aligns with the Weisfeiler-Lehman test: without injective aggregation functions, message-passing GNNs cannot distinguish certain non-isomorphic graphs, motivating more expressive variants <a class="citation" href="#xu2018powerful">(Xu et al., 2018)</a>.</p>

<p>GNNs have become a de facto choice for node classification, link prediction, and graph-level tasks across domains such as social recommendation, molecular chemistry, and traffic forecasting, thanks to their relational inductive biases and ability to handle non-Euclidean data <a class="citation" href="#zhou2020graph">(Zhou et al., 2020)</a>. Nonetheless, they can suffer from over-smoothing: as layers deepen, node embeddings converge to similar vectors, degrading discrimination capacity <a class="citation" href="#chen2020measuring">(Chen et al., 2020)</a>; this phenomenon has been formalized and mitigated through techniques such as residual connections, normalization, and graph rewiring, with recent work proving residual links provably mitigate oversmoothing rates <a class="citation" href="#chen2025residual">(Chen et al., 2025)</a>.</p>

<p>Practically, GNNs underpin breakthroughs such as AlphaFold’s Evoformer for protein folding <a class="citation" href="#jumper2021highly">(Jumper et al., 2021)</a>, spatio-temporal traffic forecasting, recommender systems exploiting user-item graphs <a class="citation" href="#wu2022graph">(Wu et al., 2022)</a>, and combinatorial solvers leveraging relational inductive biases <a class="citation" href="#battaglia2018relational">(Battaglia et al., 2018; Cappart et al., 2023)</a>, showcasing their versatility in modeling heterogeneous and dynamic graph data.</p>

<h2 id="deep-reinforcement-learning">Deep Reinforcement Learning</h2>

<p>Deep reinforcement learning formalizes sequential decision‐making as a Markov decision process, defined by a state set $S$, action set $A$, transition dynamics $P(s’ \mid s,a)$, reward function $r(s,a)$, and discount factor $\gamma$, with the goal of finding a policy $\pi$ that maximizes the expected cumulative discounted return $\mathbb{E}\big[\sum_{t=0}^\infty \gamma^t r(s_t,a_t)\big]$ <a class="citation" href="#mnih2013playing">(Mnih et al., 2013; Mnih et al., 2015; Williams, 1992)</a>. Exact solutions rely on the Bellman equations, for example</p>

\[Q^*(s,a) \;=\; \mathbb{E}\big[r(s,a) + \gamma \max_{a'} Q^*(s',a') \,\mid\,s,a\big],\]

<p>but tabular methods scale poorly when $|S|$ or $|A|$ is large <a class="citation" href="#sutton1998reinforcement">(Sutton et al., 1998)</a>.</p>

<p>Deep neural networks serve as function approximators for value functions or policies, trained via stochastic gradient descent on losses such as the temporal‐difference error</p>

\[L(\theta)=\mathbb{E}\Big[\big(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta)\big)^2\Big],\]

<p>as introduced in the Deep Q‐Network (DQN) algorithm. Actor‐critic methods generalize this approach to continuous action spaces by maintaining both a critic $Q(s,a;\theta^Q)$ and an actor $\mu(s;\theta^\mu)$, updating the latter via the deterministic policy gradient $\nabla_{\theta^\mu}J\approx\mathbb{E}[\nabla_a Q(s,a;\theta^Q)\mid_{a=\mu(s)}\nabla_{\theta^\mu}\mu(s;\theta^\mu)]$, as in Deep Deterministic Policy Gradient <a class="citation" href="#lillicrap2015continuous">(Lillicrap et al., 2015; Mnih et al., 2016)</a>.</p>

<p>The core intuition of deep reinforcement learning is that deep networks can extract abstract features from raw sensory inputs, enabling end‐to‐end learning of complex behaviors without manual feature engineering. This allows agents to directly map high‐dimensional observations to actions, as demonstrated by DQN’s human‐level performance on a suite of Atari games, where the agent learned directly from pixel inputs and sparse reward signals.</p>

<p>Despite these advances, deep RL often fails due to extreme sample inefficiency, requiring millions of interactions to converge—an untenable cost in real‐world settings where data collection is slow or expensive. Training can also be unstable because of nonstationary targets, correlated samples, and hyperparameter sensitivity, which can lead to catastrophic forgetting or divergence unless techniques like experience replay buffers and target networks are employed carefully <a class="citation" href="#franccois2018introduction">(François-Lavet et al., 2018)</a>.</p>

<p>Deep RL excels in domains with well‐defined simulators or abundant data: game playing (e.g., Atari via DQN and Go via AlphaGo and AlphaGo Zero) <a class="citation" href="#mnih2015human">(Mnih et al., 2015; Silver et al., 2016)</a>, robotics for locomotion and manipulation under physics‐based simulation and real‐world trials as surveyed in recent robotics deployments <a class="citation" href="#tang2025deep">(Tang et al., 2025)</a>, autonomous driving and resource allocation in networking, finance for portfolio optimization and algorithmic trading, and healthcare for treatment planning and personalized intervention strategies.</p>

<h2 id="ethical-and-societal-implications">Ethical and Societal Implications</h2>

<p>Algorithmic systems can perpetuate and even amplify biases present in training data, leading to unfair outcomes across demographic groups as studied extensively <a class="citation" href="#barocas2016big">(Barocas &amp; Selbst, 2016; Mehrabi et al., 2021)</a>; these biases may arise both from historical inequities encoded in data and from algorithmic design choices that inadvertently disadvantage protected groups. Efforts to protect individual privacy via formal techniques such as differential privacy provide mathematical guarantees against reidentification but introduce trade-offs with utility and require meticulous implementation—floating-point pitfalls and parameter tuning can silently undermine privacy guarantees <a class="citation" href="#dwork2014algorithmic">(Dwork et al., 2014)</a>. AI technologies also enable the rapid generation and dissemination of misinformation, and the malicious use of AI for disinformation campaigns, cyber-threat development, and political manipulation presents urgent challenges <a class="citation" href="#brundage2018malicious">(Brundage et al., 2018)</a>. The substantial computational resources demanded by training and deploying modern models entail significant environmental footprints, and the energy and carbon costs of deep learning and recommending targeted policy interventions—concerns reinforced by subsequent studies on sustainable NLP practices <a class="citation" href="#strubell2020energy">(Strubell et al., 2020)</a>. Accountability and transparency in AI systems remain paramount, as interpretability frameworks strive to render model decisions understandable, auditable, and contestable; however, the lack of consensus on definitions and evaluation metrics for interpretability underscores the need for a rigorous science of explainability <a class="citation" href="#doshi2017towards">(Doshi-Velez &amp; Kim, 2017)</a>.</p>]]></content><author><name></name></author><category term="ML" /><category term="DL" /><summary type="html"><![CDATA[This post is a very long introduction to various kinds of machine learning models and their ethical and societal implications. Jump to]]></summary></entry><entry><title type="html">Applying for Predoctoral Positions in Social Sciences and Business</title><link href="http://0.0.0.0:4000/blog/2025/02/28/predoc/" rel="alternate" type="text/html" title="Applying for Predoctoral Positions in Social Sciences and Business" /><published>2025-02-28T22:00:00-06:00</published><updated>2025-02-28T22:00:00-06:00</updated><id>http://0.0.0.0:4000/blog/2025/02/28/predoc</id><content type="html" xml:base="http://0.0.0.0:4000/blog/2025/02/28/predoc/"><![CDATA[<blockquote>
  <p><strong>WARNING</strong></p>

  <p>Examples below (in parentheses) might be outdated since positions and applications change all the time!</p>
</blockquote>

<hr />

<h4 id="on-this-page">ON THIS PAGE</h4>
<ul>
  <li><a href="#what-are-predocs">What Are Predocs?</a></li>
  <li><a href="#where-to-find-opening-positions">Where to Find Opening Positions?</a></li>
  <li><a href="#applications">Applications</a>
    <ul>
      <li><a href="#hiring-process">Hiring Process</a></li>
      <li><a href="#application-materials">Application Materials</a></li>
      <li><a href="#data-assignments">Data Assignments</a></li>
      <li><a href="#interviews">Interviews</a></li>
      <li><a href="#follow-up-emails">Follow-Up Emails?</a></li>
      <li><a href="#what-and-how-many-positions-should-i-apply-for">What and How Many Positions Should I Apply For?</a></li>
    </ul>
  </li>
  <li><a href="#should-i-do-it">Should I Do It?</a></li>
  <li><a href="#other-useful-resources">Other useful resources</a></li>
</ul>

<hr />

<p>I found the process of applying for predocs quite painful (actually more than graduate programs), so I hope this blog helps! Don’t be too anxious! :rocket:</p>

<hr />

<h2 id="what-are-predocs">What Are Predocs?</h2>

<p>A <strong>predoc</strong> (short for <em>pre-doctoral fellow</em>) is someone engaged in academic research before starting a PhD program. The term is commonly used for individuals working as research assistants or fellows at universities, research institutions, or labs. Predocs typically gain research experience <del>good recommendation letters</del> to strengthen their PhD applications.</p>

<p>Depending on the institution, predocs may hold titles such as research fellow, research scientist, research associate, research professional, research technician, or research assistant.</p>

<h2 id="where-to-find-opening-positions">Where to Find Opening Positions?</h2>

<p>The most important source of open positions is <strong>your connections</strong>. For example, you can directly ask your supervisor if they know of any openings through their private network. Using personal connections maximizes your chances of securing an ideal position—one that aligns with your research interests—while also saving time and effort.</p>

<p>Beyond personal connections, key online resources for predoc openings include:</p>

<ul>
  <li><a href="https://www.predoc.org/opportunities">Listings on predoc.org</a></li>
  <li><a href="https://www.nber.org/career-resources/research-assistant-positions-not-nber">Job posts on NBER</a></li>
  <li><a href="https://x.com/econ_ra?lang=en">Retweets of Econ RA Listings</a></li>
</ul>

<p>During the main application season (<strong>July to the following August</strong>), these sources may list over 150 positions. Employers include:</p>

<ul>
  <li><strong>Research institutions</strong> (e.g., Children’s National Hospital)</li>
  <li><strong>Universities</strong> (e.g., Northwestern University)</li>
  <li><strong>Private labs</strong> (e.g., GovAI)</li>
  <li><strong>Companies</strong> (e.g., Integra FEC)</li>
  <li><strong>Federal/governmental institutes</strong> (e.g., The Federal Reserve System)</li>
  <li><strong>NGOs</strong> (e.g., the IMF and World Bank)</li>
</ul>

<p>Most positions are based in the U.S., but opportunities exist worldwide. Generally, <strong>economics-focused positions</strong> are posted earlier (July to December), while <strong>business school positions</strong> are posted later, beginning in January. However, some business schools—such as Stanford GSB, Northwestern Kellogg, and Columbia Business School—may post openings earlier. So if you don’t get an offer (or even see an interesting position) before February, don’t panic, more are to come!</p>

<h4 id="considerations-for-international-candidates">Considerations for International Candidates</h4>

<p>If you’re applying from outside the employer’s country, it’s crucial to check visa sponsorship requirements and citizenship/residency conditions or so. For example, some U.S.–based positions may require candidates to have lived in the U.S. for at least three of the past five years (generally due to involvment of sensitive data).</p>

<h2 id="applications">Applications</h2>

<p>The first step in applying for a predoc position typically involves submitting application materials, which may include:</p>

<ul>
  <li><strong>CV</strong> (usually two pages)</li>
  <li><strong>Cover letter</strong> (typically one page)</li>
  <li><strong>Unofficial transcripts</strong> (for all degree programs and sometimes individual courses)</li>
  <li><strong>Writing/coding samples or GitHub portfolio</strong> (if applicable)</li>
</ul>

<p>Institutions have different application submission methods:</p>

<ul>
  <li><strong>Public application systems</strong> (e.g., Empirical Fellow at Northwestern Kellogg, J-PAL)</li>
  <li><strong>Google Forms</strong> (e.g., UChicago Booth)</li>
  <li><strong>Email submission</strong> (e.g., Vancouver School of Economics at UBC, where applicants compile materials into a single PDF)</li>
</ul>

<p>It is crucial to carefully read and follow the specific application instructions for each position.</p>

<h3 id="hiring-process">Hiring Process</h3>

<p>The hiring process usually consists of two to three rounds:</p>

<ol>
  <li>Initial application submission</li>
  <li>Screening process for selected candidates:
    <ul>
      <li>An initial interview (not so common, often at HBS, Kellogg, Wharton)</li>
      <li>A data assignment</li>
    </ul>
  </li>
  <li>Final-round interviews:
    <ul>
      <li>These typically follow the data assignment, though in some cases, the assignment may follow the interview.</li>
      <li>Some positions may offer fly-out opportunities for selected candidates (e.g., Real Estate Center at The Wharton School).</li>
    </ul>
  </li>
</ol>

<h3 id="important-notes-for-international-candidates">Important Notes for International Candidates</h3>

<p>If you do not regularly use email, take extra precautions:</p>

<ul>
  <li><strong>Check your inbox and spam folder frequently</strong>—important updates (such as a data assignment or interview invitation) arrive via email.
    <ul>
      <li>:bulb: Check your email at least once per day to avoid missing deadlines.</li>
    </ul>
  </li>
  <li><strong>Ensure your email account can receive external (potentially foreign) messages.</strong>
    <ul>
      <li>:pushpin: Gmail is generally recommended.</li>
    </ul>
  </li>
</ul>

<h3 id="application-materials">Application Materials</h3>

<p>Application materials generally include:</p>

<ul>
  <li><strong>CV</strong> (typically two pages)</li>
  <li><strong>Cover letter</strong> (typically one page)</li>
  <li><strong>Unofficial transcripts</strong> (for all degree programs and sometimes individual courses)</li>
  <li><strong>Coding/GitHub samples</strong> (optional but recommended)</li>
  <li><strong>Writing samples</strong> (optional but recommended)</li>
  <li><strong>Recommendation letters</strong> (often required later in the process)</li>
</ul>

<h4 id="cv">CV</h4>

<p>Your CV is a crucial part of your application package. Prioritize research experience, but also include relevant experience such as internships and volunteer work.</p>

<p>:bulb: <strong>Tips for a strong CV:</strong></p>

<ul>
  <li>If you are from an English-speaking university, ask your career services office for a CV template.</li>
  <li>Proofread your CV with career services staff or peers.</li>
  <li>Use concise, clear sentences that are easy to scan quickly.</li>
  <li>Add an overview of skills (e.g., programming languages, software proficiency) at the beginning.</li>
  <li>Perform a spell check!</li>
</ul>

<p>:pushpin: <strong>Follow specific instructions</strong> if provided. Some positions require additional details, such as GPA or referees—always adhere to these guidelines.</p>

<h4 id="cover-letter">Cover Letter</h4>

<p>If you are from an English-speaking university, ask your career services office for a cover letter template.</p>

<p>:bulb: <strong>Key considerations for cover letters:</strong></p>

<ul>
  <li>Most positions require a cover letter.</li>
  <li>Many positions specify content requirements—follow these instructions carefully! Otherwise, your application may be overlooked or dismissed.</li>
  <li>Common requirements include:
    <ul>
      <li>Programming language proficiency</li>
      <li>Research area preferences or preferred supervisors</li>
      <li>Referees</li>
      <li>Relevant experience</li>
    </ul>
  </li>
</ul>

<p>:pushpin: <strong>Tailoring is essential!</strong><br />
The minimum tailoring effort should involve:</p>

<ul>
  <li>Uploading the job post, the supervisor’s personal website/Google Scholar page, and your cover letter prototype to ChatGPT for refinement.</li>
  <li>For your top-choice positions, invest additional effort in customization!</li>
</ul>

<h4 id="optional-codinggithub-samples">(Optional) Coding/GitHub Samples</h4>

<p>You don’t have to submit coding samples if you don’t have something that meets the job’s requirements. However, be cautious—if the job requires proficiency in Stata and you lack relevant samples, consider how you will demonstrate competency elsewhere. Some candidates have received offers without submitting a coding sample, as data assignments often serve as a coding assessment.</p>

<p>:bulb: <strong>If you submit a sample, ensure:</strong></p>

<ul>
  <li>The code is well-formatted and has sufficient comments for readability.</li>
  <li>Inputs and outputs (or an example dataset) are included.</li>
  <li>If submitting a large Python project, include:
    <ul>
      <li>A requirements file listing necessary packages.</li>
      <li>Separate scripts for functions and execution commands.</li>
    </ul>
  </li>
</ul>

<h4 id="optional-writing-samples">(Optional) Writing Samples</h4>

<p>A writing sample (e.g., a term paper, honors thesis, or publication) is highly recommended.</p>

<p>:pushpin: <strong>Key points:</strong></p>

<ul>
  <li>The main goal is not to assess technical complexity but clarity of communication in English.</li>
  <li>Poorly written samples—especially for non-native English speakers—can lower the chance of being hired.</li>
  <li>Ensure proper proofreading and polishing before submission.</li>
</ul>

<h4 id="recommendation-letters">Recommendation Letters</h4>

<p>:pushpin: When are recs required?</p>

<ul>
  <li>Only 20–30% of positions require recommendation letters at the beginning of the hiring process.</li>
  <li>For most positions, recs are requested at the final stage as an administrative or confirmatory step.</li>
</ul>

<h3 id="data-assignments">Data Assignments</h3>

<p>“Data assignments” can cover a wide range of tasks, including:</p>

<ul>
  <li>Data preprocessing</li>
  <li>Econometric applications</li>
  <li>Unstructured data processing</li>
  <li>Machine learning applications</li>
  <li>Mathematical proofs and theory questions</li>
  <li>Summarizing a recent research paper</li>
</ul>

<p>Most assignments allow completion in any programming language, but some may require or recommend using Stata or Python</p>

<ul>
  <li>Python is often used for unstructured data processing or machine learning applications (especially using large language models).</li>
  <li>Stata is common in applied economics research. If you plan to apply for Stata-heavy roles, ensure you have access to a licensed version before applying.</li>
</ul>

<p>:pushpin: <strong>Submission Guidelines:</strong></p>

<ul>
  <li>Assignments typically require both a report and the code used for analysis.</li>
  <li>The report should be clearly written in LaTeX with proper typesetting (e.g., no equations in the margins).</li>
  <li>Code should be:
    <ul>
      <li>Well-commented and structured for readability.</li>
      <li>Optimized (e.g., avoid 10 nested loops)</li>
      <li>If submitting a large Python project, include:
        <ul>
          <li>A requirements file listing necessary packages.</li>
          <li>Separate scripts for functions/passcodes.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>One goal of these assignments is to evaluate coding habits—clean, efficient, and readable code is essential!</p>

<h3 id="interviews">Interviews</h3>

<h4 id="initial-interview">Initial Interview</h4>

<p>The initial interview typically aims to:</p>

<ul>
  <li>Assess fit with the research team.</li>
  <li>Recommend candidates to appropriate faculty or research groups.</li>
</ul>

<h4 id="final-round-interview">Final-Round Interview</h4>

<p>The last-round interview is the most critical stage of the hiring process. This interview is usually conducted:</p>

<ul>
  <li>by the supervisor you may directly work with.</li>
  <li>over Zoom or a phone call—ensure a quiet, private space with no background noise.</li>
  <li>on schedule—be punctual!</li>
</ul>

<p>:pushpin: <strong>Preparation Tips:</strong></p>

<ul>
  <li>Review the interviewer’s CV and research papers to understand their research themes.</li>
  <li>Look at their “Work in Progress” section in CV to anticipate potential tasks.</li>
  <li>Prepare to discuss your research/work experience and data assignment smoothly especially if English is not your first language.</li>
  <li>Consider drafting a self-introduction to practice fluency and confidence.</li>
</ul>

<h4 id="what-determines-success">What Determines Success?</h4>

<p>A successful interview hinges on meeting the supervisor’s expectations for their projects. In some cases:</p>

<ul>
  <li>If a candidate is a strong match, they may interview just one person before sending an offer.</li>
  <li>In other cases, supervisors may interview 10+ candidates before making a decision.</li>
</ul>

<h3 id="follow-up-emails">Follow-Up Emails?</h3>

<p>Follow-up emails can demonstrate initiative and commitment. I personally only sent follow-up emails after the final round for my favorite positions.</p>

<p>:pushpin: If you send a follow-up email:</p>

<ul>
  <li>Make it concise—long emails are unnecessary.</li>
  <li>Tailor it to the position—mention specifics.</li>
  <li>Proofread for grammar and spelling errors.</li>
</ul>

<p>A poorly written email won’t help, so if in doubt, it’s better not to send one!</p>

<h3 id="what-and-how-many-positions-should-i-apply-for">What and How Many Positions Should I Apply For?</h3>

<p>There is a bias-variance trade-off involved in the application process:</p>

<ul>
  <li>
    <p>If you only apply to positions that are a strong fit, you save effort. However, during the hiring process (data assignments or interviews), you may struggle compared to candidates who have applied to many positions and gained experience.</p>

    <ul>
      <li>For example, you might get nervous in interviews or mismanage time during a two-hour data assignment.</li>
      <li>This results in high variance in your performance due to limited exposure (small sample size).</li>
    </ul>
  </li>
  <li>
    <p>If you apply to many positions, you gain experience (by making and learning from mistakes) and reduce variance in your performance.</p>
    <ul>
      <li>However, your fundamental fit for positions (bias) does not necessarily improve.</li>
      <li>You may also waste time on applications and end up in a role that isn’t an ideal match, often due to risk aversion.</li>
    </ul>
  </li>
</ul>

<p>However, if you consider yourself a non-parametric model (for example, if you don’t know exactly what you like, aren’t sure what you’re especially good at, or simply want a job), then a larger sample size reduces bias as well. :point_right: Applying to more positions is recommended! Increasing the number of applications helps you explore possibilities, gain experience, and ultimately refine your preferences.</p>

<h2 id="should-i-do-it">Should I Do It?</h2>

<p>At some point, you may ask yourself: <strong>Should I really apply for a predoc?</strong></p>

<p>The best approach is often to apply for a combination of predocs and PhD programs with weights of your choice.</p>

<ul>
  <li>If your goal is not a top PhD program (as per your own definition), a predoc may not be necessary.</li>
  <li>
    <p>However, applying to a range of predocs while also applying to:</p>

    <ul>
      <li>Highly ranked PhD programs (for strong candidates),</li>
      <li>Lower-tier PhD programs or Master’s programs (if budget allows),</li>
      <li>Or some combination of these</li>
    </ul>

    <p>…is a strategy that lowers the risk of getting no offers at all (recall microeconomic theories of risk).</p>
  </li>
</ul>

<p>From an individual perspective, the optimal approach is:</p>

<ol>
  <li>Maximize your choice set by applying to as many programs as possible.</li>
  <li>Evaluate your options wisely once offers come in.</li>
</ol>

<p>The downside?</p>

<ul>
  <li>This strategy isn’t great for market equilibrium—it makes the process more competitive and exhausting for everyone.</li>
  <li>Unfortunately, this is a prisoners’ dilemma—everyone applies for more programs to maximize their utility (myself included!).</li>
</ul>

<p>In the end, balancing effort, risk, and personal career goals is key. :rocket:</p>

<h2 id="other-useful-resources">Other useful resources</h2>

<ul>
  <li><a href="https://github.com/Alalalalaki/Guide2EconRA?tab=readme-ov-file">Guide2EconRA</a>: Links to many lectures, tutorials, advice/tips.</li>
  <li><a href="https://raguide.github.io">Econ RA Guide</a>: Overview, applications, positions.</li>
  <li><a href="https://qlquanle.github.io/links">A collection of links by Quan Le</a>: Blogs, advice, opportunities, overview of grad school/research in econ.</li>
  <li><a href="https://github.com/esoclabprinceton/ESOC-Predoc-Training">FDR Pre-Doctoral Training Curriculum</a>: Resources for predocs at the Princeton Empirical Studies of Conflict Lab, including programming, statistics, GIS, causal inference, typesetting, data pre-processing and visualization, and text methods.</li>
</ul>]]></content><author><name></name></author><category term="Applications" /><summary type="html"><![CDATA[WARNING Examples below (in parentheses) might be outdated since positions and applications change all the time!]]></summary></entry><entry><title type="html">Some fonts in LaTeX</title><link href="http://0.0.0.0:4000/blog/2024/05/15/fonts/" rel="alternate" type="text/html" title="Some fonts in LaTeX" /><published>2024-05-15T23:00:00-05:00</published><updated>2024-05-15T23:00:00-05:00</updated><id>http://0.0.0.0:4000/blog/2024/05/15/fonts</id><content type="html" xml:base="http://0.0.0.0:4000/blog/2024/05/15/fonts/"><![CDATA[<h4 id="on-this-page">ON THIS PAGE</h4>

<ul>
  <li><a href="#times-new-roman">Times New Roman</a></li>
  <li><a href="#palatino">Palatino</a></li>
  <li><a href="#garamond">Garamond</a></li>
  <li><a href="#erewhon">Erewhon</a></li>
  <li><a href="#georgia">Georgia</a></li>
  <li><a href="#references-on-ctan">References on CTAN</a></li>
</ul>

<hr />

<p>LaTeX is probably familiar to most people who do quantitative work. Sometimes we might want to use different fonts, to make our documents look nicer. This post introduces some commonly used fonts with their respective math fonts in LaTeX and how to load them with PDFLaTeX. Also, there is a picture of example output of each font generated by the <code class="language-plaintext highlighter-rouge">duckuments</code> package.</p>

<hr />

<h2 id="times-new-roman">Times New Roman</h2>

<p>Times New Roman–type fonts are used almost everywhere, formally or informally. To use the font globally, put the following in the preamble (i.e., after <code class="language-plaintext highlighter-rouge">\documentclass[]{}</code> and before <code class="language-plaintext highlighter-rouge">\begin{document}</code>):</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="p">{</span>newtxtext, newtxmath<span class="p">}</span>
</code></pre></div></div>

<p>An example paragraph in this font:</p>

<p><img src="/assets/img/fonts/newtx.jpg" alt="Times New Roman" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h2 id="palatino">Palatino</h2>

<p>Similarly, use the <code class="language-plaintext highlighter-rouge">newpx</code> packages as below,</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="p">{</span>newpxtext,newpxmath<span class="p">}</span>
</code></pre></div></div>

<p>An example paragraph in this font:</p>

<p><img src="/assets/img/fonts/newpx.jpg" alt="Palatino" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h2 id="garamond">Garamond</h2>

<p>EB Garamond is recommended generally. Use it by putting the following into the preamble</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="na">[cmintegrals,cmbraces]</span><span class="p">{</span>newtxmath<span class="p">}</span>
<span class="k">\usepackage</span><span class="p">{</span>ebgaramond-maths<span class="p">}</span>
<span class="k">\usepackage</span><span class="na">[T1]</span><span class="p">{</span>fontenc<span class="p">}</span>
</code></pre></div></div>

<p>An example paragraph in this font:</p>

<p><img src="/assets/img/fonts/ebg.jpg" alt="EB Garamond" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h2 id="erewhon">Erewhon</h2>

<p>To use Erewhon, use</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="na">[space]</span><span class="p">{</span>erewhon<span class="p">}</span>
<span class="k">\usepackage</span><span class="na">[type1,scaled=.95]</span><span class="p">{</span>cabin<span class="p">}</span>
<span class="k">\usepackage</span><span class="na">[utopia,vvarbb]</span><span class="p">{</span>newtxmath<span class="p">}</span>
</code></pre></div></div>

<p>which gives</p>

<p><img src="/assets/img/fonts/erewhon.jpg" alt="Erewhon" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h2 id="georgia">Georgia</h2>

<p>This solution comes from the <a href="https://tex.stackexchange.com/a/464586/317380">an answer on StackExchange</a>. To use the font in this way with math, use LuaLaTeX instead of PDFLaTeX! The example output comes from the original answer.</p>

<p><img src="/assets/img/fonts/g.png" alt="Georgia" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>
<figcaption style="font-size:0.9rem; color:#555; text-align:center; margin-top:0.5rem;">
Source: <a href="https://i.stack.imgur.com/3uEFd.png">https://i.stack.imgur.com/3uEFd.png</a>, <a href="https://tex.stackexchange.com/a/464586/317380">https://tex.stackexchange.com/a/464586/317380</a>.
</figcaption>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="na">[math-style=ISO]</span><span class="p">{</span>unicode-math<span class="p">}</span>
<span class="k">\usepackage</span><span class="p">{</span>microtype<span class="p">}</span>

<span class="k">\defaultfontfeatures</span><span class="p">{</span>Scale=MatchLowercase<span class="p">}</span>
<span class="k">\setmainfont</span><span class="p">{</span>Georgia<span class="p">}</span>[Scale = 1.0]
<span class="k">\setmathfont</span><span class="p">{</span>Asana Math<span class="p">}</span>
<span class="k">\setmathfont</span><span class="na">[range={up,`∏,`∑,`∙,`√,`∞}, script-features={}, sscript-features={}]</span><span class="p">{</span>Georgia<span class="p">}</span>
<span class="k">\setmathfont</span><span class="na">[range=it,script-features={}, sscript-features={}]</span><span class="p">{</span>Georgia Italic<span class="p">}</span>
<span class="k">\setmathfont</span><span class="na">[range=bfup,script-features={}, sscript-features={}]</span><span class="p">{</span>Georgia Bold<span class="p">}</span>
<span class="k">\setmathfont</span><span class="na">[range=bfit,script-features={}, sscript-features={}]</span><span class="p">{</span>Georgia Bold Italic<span class="p">}</span>
</code></pre></div></div>

<hr />

<h2 id="references-on-ctan">References on CTAN</h2>
<p>CTAN stands for The Comprehensive TeX Archive Network, and it has all kinds of packages in TeX. As of the written day, there are 6542 packages, most of which can be downloaded and used immediately. This list below, in alphabetical order, contains the links of the aforementioned packages in CTAN.</p>

<ul>
  <li>cabin: <a href="https://ctan.org/pkg/cabin?lang=en">https://ctan.org/pkg/cabin?lang=en</a></li>
  <li>duckuments: <a href="https://ctan.org/pkg/duckuments?lang=en">https://ctan.org/pkg/duckuments?lang=en</a></li>
  <li>ebgaramond-maths: <a href="https://ctan.org/pkg/ebgaramond-maths?lang=en">https://ctan.org/pkg/ebgaramond-maths?lang=en</a></li>
  <li>erewhon: <a href="https://ctan.org/pkg/erewhon?lang=en">https://ctan.org/pkg/erewhon?lang=en</a></li>
  <li>fontenc: <a href="https://ctan.org/pkg/fontenc?lang=en">https://ctan.org/pkg/fontenc?lang=en</a></li>
  <li>newpx: <a href="https://ctan.org/pkg/newpx?lang=en">https://ctan.org/pkg/newpx?lang=en</a></li>
  <li>newtx: <a href="https://ctan.org/pkg/newtx?lang=en">https://ctan.org/pkg/newtx?lang=en</a></li>
  <li>unicode-math: <a href="https://ctan.org/pkg/unicode-math?lang=en">https://ctan.org/pkg/unicode-math?lang=en</a></li>
</ul>]]></content><author><name></name></author><category term="Typesetting" /><category term="LaTeX" /><summary type="html"><![CDATA[ON THIS PAGE]]></summary></entry><entry><title type="html">Comparative Judgment, Pairwise Comparison, and Bradley–Terry Model</title><link href="http://0.0.0.0:4000/blog/2023/12/12/cj/" rel="alternate" type="text/html" title="Comparative Judgment, Pairwise Comparison, and Bradley–Terry Model" /><published>2023-12-12T00:00:00-06:00</published><updated>2023-12-12T00:00:00-06:00</updated><id>http://0.0.0.0:4000/blog/2023/12/12/cj</id><content type="html" xml:base="http://0.0.0.0:4000/blog/2023/12/12/cj/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Comparative judgment involves using pairwise comparisons to access the abilities of the compared objects. This is widely applied in the real world, e.g., sports science, psychology, and education. For example, many sport games (e.g., basketball, football, chess) involve two players/teams competing against each other each time. The resulting data can be used in comparative judgment.</p>

<h2 id="bradleyterry-model">Bradley–Terry Model</h2>

<p>The Bradley–Terry model <a class="citation" href="#btorigin">(Bradley &amp; Terry, 1952)</a> is commonly applied in comparative judgment. It assumes a set \(\mathcal{P}=\left\{1,\dots,p\right\}\) in which every component is a “player” compared with another by some judges, with \(\alpha_i/\alpha_j\) being the probability of \(i\) beating \(j\). In functional form, the model seeks to estimate</p>

\[\text{Logit}(\pi_{ij}) = \lambda_i-\lambda_j,\]

<p>where \(\lambda_i=\log\left(\alpha_i\right)\), \(\alpha_i\) and \(\lambda_i\) are the ability score and log ability score of player \(i\) in \(\mathcal{P}\) respectively and \(\pi_{ij}\) is the probability of \(i\) beats \(j\). However, in this context, none of the above variables are known in practice. By the nature of logistic regression, the total number of times that \(i\) beats \(j\), \(Y_{ij}\), is assumed to follow the binomial distribution:</p>

\[Y_{ij}\sim\text{Binomial}\left(n_{ij},\pi_{ij}\right),\]

<p>where \(n_{ij}=n_{ji}\) is the number of times that \(i\) and \(j\) are compared against each other. This unstructured model could be further extended to a structured version, in which</p>

\[\lambda_i=\sum^p_{q=1}\beta_qx_{iq}+\varepsilon_i,\]

<p>where \(x_q\)’s are explanatory variables and \(\varepsilon_i\sim\mathcal{N}\left(0,\sigma^2\right)\) is the random error term. Both fixed and random effects could be used for estimation. The explanatory variables could be attributed to the players being compared (player-specific), to the judge (contest-specific), or to the specific comparison (contest-specific).</p>

<h2 id="bayesian-spatial-bradleyterry-model">Bayesian Spatial Bradley–Terry Model</h2>

<p>The Bayesian Spatial Bradley–Terry model is specific to spatial data, allowing the learning of abilities from nearby players via clustering <a class="citation" href="#rowland2022">(Seymour et al., 2023; Seymour et al., 2022)</a>. This model is useful for assessing issues related to human right abuses, such as forced marriage, female genital mutilation, human trafficking, and online child sexual exploitation and abuse.</p>

<h2 id="model-fitting">Model fitting</h2>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">BradleyTerry2</code> package in <code class="language-plaintext highlighter-rouge">R</code> for the Bradley–Terry model: <a href="https://github.com/hturner/BradleyTerry2">https://github.com/hturner/BradleyTerry2</a> <a class="citation" href="#firth2012">(Turner &amp; Firth, 2012)</a>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">BSBT</code> package in <code class="language-plaintext highlighter-rouge">R</code> for the Bayesian Spatial Bradley–Terry model: <a href="https://github.com/rowlandseymour/BSBT">https://github.com/rowlandseymour/BSBT</a> <a class="citation" href="#bsbt">(Seymour et al., 2022)</a>.</li>
</ul>]]></content><author><name></name></author><category term="Statistics" /><category term="ML" /><summary type="html"><![CDATA[Introduction Comparative judgment involves using pairwise comparisons to access the abilities of the compared objects. This is widely applied in the real world, e.g., sports science, psychology, and education. For example, many sport games (e.g., basketball, football, chess) involve two players/teams competing against each other each time. The resulting data can be used in comparative judgment. Bradley–Terry Model The Bradley–Terry model (Bradley &amp; Terry, 1952) is commonly applied in comparative judgment. It assumes a set \(\mathcal{P}=\left\{1,\dots,p\right\}\) in which every component is a “player” compared with another by some judges, with \(\alpha_i/\alpha_j\) being the probability of \(i\) beating \(j\). In functional form, the model seeks to estimate \[\text{Logit}(\pi_{ij}) = \lambda_i-\lambda_j,\] where \(\lambda_i=\log\left(\alpha_i\right)\), \(\alpha_i\) and \(\lambda_i\) are the ability score and log ability score of player \(i\) in \(\mathcal{P}\) respectively and \(\pi_{ij}\) is the probability of \(i\) beats \(j\). However, in this context, none of the above variables are known in practice. By the nature of logistic regression, the total number of times that \(i\) beats \(j\), \(Y_{ij}\), is assumed to follow the binomial distribution: \[Y_{ij}\sim\text{Binomial}\left(n_{ij},\pi_{ij}\right),\] where \(n_{ij}=n_{ji}\) is the number of times that \(i\) and \(j\) are compared against each other. This unstructured model could be further extended to a structured version, in which \[\lambda_i=\sum^p_{q=1}\beta_qx_{iq}+\varepsilon_i,\] where \(x_q\)’s are explanatory variables and \(\varepsilon_i\sim\mathcal{N}\left(0,\sigma^2\right)\) is the random error term. Both fixed and random effects could be used for estimation. The explanatory variables could be attributed to the players being compared (player-specific), to the judge (contest-specific), or to the specific comparison (contest-specific). Bayesian Spatial Bradley–Terry Model The Bayesian Spatial Bradley–Terry model is specific to spatial data, allowing the learning of abilities from nearby players via clustering (Seymour et al., 2023; Seymour et al., 2022). This model is useful for assessing issues related to human right abuses, such as forced marriage, female genital mutilation, human trafficking, and online child sexual exploitation and abuse. Model fitting The BradleyTerry2 package in R for the Bradley–Terry model: https://github.com/hturner/BradleyTerry2 (Turner &amp; Firth, 2012). The BSBT package in R for the Bayesian Spatial Bradley–Terry model: https://github.com/rowlandseymour/BSBT (Seymour et al., 2022).]]></summary></entry><entry><title type="html">Learn LaTeX Quickly</title><link href="http://0.0.0.0:4000/blog/2023/01/14/tex/" rel="alternate" type="text/html" title="Learn LaTeX Quickly" /><published>2023-01-14T22:00:00-06:00</published><updated>2023-01-14T22:00:00-06:00</updated><id>http://0.0.0.0:4000/blog/2023/01/14/tex</id><content type="html" xml:base="http://0.0.0.0:4000/blog/2023/01/14/tex/"><![CDATA[<hr />

<center><b><p style="font-size: 13pt;"> 
<a href="/assets/pdf/tex/tex.pdf">PDF Version Available Here</a> (It might be more useful!)
</p></b></center>

<hr />

<p>Below is the outline of this blog:</p>

<ul>
  <li><a href="#figures">Figures</a></li>
  <li><a href="#tables">Tables</a></li>
  <li><a href="#equations">Equations</a></li>
  <li><a href="#theorems-and-definitions">Theorems and Definitions</a></li>
  <li><a href="#code">Code</a></li>
  <li><a href="#algorithms">Algorithms</a></li>
  <li><a href="#layout">Layout</a></li>
  <li><a href="#tikz">TikZ</a></li>
</ul>

<h2 id="figures">Figures</h2>

<p>Images in LaTeX documents should be placed in floating environments such as <code class="language-plaintext highlighter-rouge">figure</code>. Below are examples of one- through four-image figures in the same <code class="language-plaintext highlighter-rouge">figure</code> environment. Note that packages <code class="language-plaintext highlighter-rouge">graphicx</code>, <code class="language-plaintext highlighter-rouge">caption</code>, and <code class="language-plaintext highlighter-rouge">subcaption</code> should be loaded.</p>

<h3 id="one-image-figure">One-image figure</h3>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{figure}</span>[htbp]
  <span class="k">\centering</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.5\textwidth]</span><span class="p">{</span>example-image<span class="p">}</span>
  <span class="k">\caption</span><span class="p">{</span>An one-image figure.<span class="p">}</span>
  <span class="k">\label</span><span class="p">{</span>fig:1-image<span class="p">}</span>
<span class="nt">\end{figure}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/1i.png" alt="One image" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="two-image-figure">Two-image figure</h3>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{figure}</span>[htbp]
  <span class="k">\centering</span>
  <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
    <span class="k">\centering</span>
    <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-a<span class="p">}</span>
    <span class="k">\caption</span><span class="p">{</span>Image A.<span class="p">}</span>
    <span class="k">\label</span><span class="p">{</span>fig:2-image-a<span class="p">}</span>
  <span class="nt">\end{subfigure}</span>
  <span class="k">\hfill</span>
  <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
    <span class="k">\centering</span>
    <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-b<span class="p">}</span>
    <span class="k">\caption</span><span class="p">{</span>Image B.<span class="p">}</span>
    <span class="k">\label</span><span class="p">{</span>fig:2-image-b<span class="p">}</span>
  <span class="nt">\end{subfigure}</span>
  <span class="k">\caption</span><span class="p">{</span>A two-image figure<span class="p">}</span>
  <span class="k">\label</span><span class="p">{</span>fig:2-image<span class="p">}</span>
<span class="nt">\end{figure}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/2i.png" alt="Two images" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="three-image-figure">Three-image figure</h3>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{figure}</span>[htbp]
     <span class="k">\centering</span>
     <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.3<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-a<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image A.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:3-image-a<span class="p">}</span>
     <span class="nt">\end{subfigure}</span>
     <span class="k">\hfill</span>
     <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.3<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-b<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image B.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:3-image-b<span class="p">}</span>
     <span class="nt">\end{subfigure}</span>
     <span class="k">\hfill</span>
     <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.3<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-c<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image C.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:3-image-c<span class="p">}</span>
     <span class="nt">\end{subfigure}</span>
     <span class="k">\caption</span><span class="p">{</span>A three-image figure.<span class="p">}</span>
     <span class="k">\label</span><span class="p">{</span>fig:3-image<span class="p">}</span>
<span class="nt">\end{figure}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/3i.png" alt="Three images" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="four-image-figure">Four-image figure</h3>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{figure}</span>[htbp]
     <span class="k">\centering</span>
     <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-a<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image A.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:4-image-a<span class="p">}</span>
     <span class="nt">\end{subfigure}</span>
     <span class="k">\hfill</span>
     <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-b<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image B.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:4-image-b<span class="p">}</span>
     <span class="nt">\end{subfigure}</span>
     <span class="k">\hfill</span>
     <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-c<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image C.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:4-image-c<span class="p">}</span>
    <span class="nt">\end{subfigure}</span>
    <span class="k">\hfill</span>
    <span class="nt">\begin{subfigure}</span>[b]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
         <span class="k">\centering</span>
         <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="p">{</span>example-image-plain<span class="p">}</span>
         <span class="k">\caption</span><span class="p">{</span>Image D.<span class="p">}</span>
         <span class="k">\label</span><span class="p">{</span>fig:4-image-d<span class="p">}</span>
    <span class="nt">\end{subfigure}</span>
    <span class="k">\caption</span><span class="p">{</span>A four-image figure.<span class="p">}</span>
    <span class="k">\label</span><span class="p">{</span>fig:4-image<span class="p">}</span>
<span class="nt">\end{figure}</span>
</code></pre></div></div>
<p><img src="/assets/pdf/tex/4i.png" alt="Four images" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<hr />

<h2 id="tables">Tables</h2>

<p>Tables live in a <code class="language-plaintext highlighter-rouge">table</code> float and can be simple or composed of subtables.</p>

<h3 id="basic-table">Basic table</h3>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{table}</span>[htbp]
<span class="k">\centering</span>
<span class="nt">\begin{tabular}</span><span class="p">{</span>l | l | l<span class="p">}</span>
<span class="k">\hline</span>
A <span class="p">&amp;</span> B <span class="p">&amp;</span> C <span class="k">\\</span>
<span class="k">\hline</span>
1 <span class="p">&amp;</span> 2 <span class="p">&amp;</span> 3 <span class="k">\\</span>
4 <span class="p">&amp;</span> 5 <span class="p">&amp;</span> 6 <span class="k">\\</span>
<span class="k">\hline</span>
<span class="nt">\end{tabular}</span>
<span class="k">\caption</span><span class="p">{</span>A very basic table<span class="p">}</span>
<span class="k">\label</span><span class="p">{</span>tab:basic<span class="p">}</span>
<span class="nt">\end{table}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/basictab.png" alt="Basic table" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="subtables">Subtables</h3>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{table}</span>[htbp]
    <span class="nt">\begin{subtable}</span>[h]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
        <span class="k">\centering</span>
        <span class="nt">\begin{tabular}</span><span class="p">{</span>l | l | l<span class="p">}</span>
        Day <span class="p">&amp;</span> Max Temp <span class="p">&amp;</span> Min Temp <span class="k">\\</span>
        <span class="k">\hline</span> <span class="k">\hline</span>
        Mon <span class="p">&amp;</span> 20 <span class="p">&amp;</span> 13<span class="k">\\</span>
        Tue <span class="p">&amp;</span> 22 <span class="p">&amp;</span> 14<span class="k">\\</span>
        Wed <span class="p">&amp;</span> 23 <span class="p">&amp;</span> 12<span class="k">\\</span>
        Thurs <span class="p">&amp;</span> 25 <span class="p">&amp;</span> 13<span class="k">\\</span>
        Fri <span class="p">&amp;</span> 18 <span class="p">&amp;</span> 7<span class="k">\\</span>
        Sat <span class="p">&amp;</span> 15 <span class="p">&amp;</span> 13<span class="k">\\</span>
        Sun <span class="p">&amp;</span> 20 <span class="p">&amp;</span> 13
       <span class="nt">\end{tabular}</span>
       <span class="k">\caption</span><span class="p">{</span>First Week<span class="p">}</span>
       <span class="k">\label</span><span class="p">{</span>tab:week1<span class="p">}</span>
    <span class="nt">\end{subtable}</span>
    <span class="k">\hfill</span>
    <span class="nt">\begin{subtable}</span>[h]<span class="p">{</span>0.45<span class="k">\textwidth</span><span class="p">}</span>
        <span class="k">\centering</span>
        <span class="nt">\begin{tabular}</span><span class="p">{</span>l | l | l<span class="p">}</span>
        Day <span class="p">&amp;</span> Max Temp <span class="p">&amp;</span> Min Temp <span class="k">\\</span>
        <span class="k">\hline</span> <span class="k">\hline</span>
        Mon <span class="p">&amp;</span> 17 <span class="p">&amp;</span> 11<span class="k">\\</span>
        Tue <span class="p">&amp;</span> 16 <span class="p">&amp;</span> 10<span class="k">\\</span>
        Wed <span class="p">&amp;</span> 14 <span class="p">&amp;</span> 8<span class="k">\\</span>
        Thurs <span class="p">&amp;</span> 12 <span class="p">&amp;</span> 5<span class="k">\\</span>
        Fri <span class="p">&amp;</span> 15 <span class="p">&amp;</span> 7<span class="k">\\</span>
        Sat <span class="p">&amp;</span> 16 <span class="p">&amp;</span> 12<span class="k">\\</span>
        Sun <span class="p">&amp;</span> 15 <span class="p">&amp;</span> 9
        <span class="nt">\end{tabular}</span>
        <span class="k">\caption</span><span class="p">{</span>Second Week<span class="p">}</span>
        <span class="k">\label</span><span class="p">{</span>tab:week2<span class="p">}</span>
     <span class="nt">\end{subtable}</span>
     <span class="k">\caption</span><span class="p">{</span>Max and min temps recorded in the first two weeks of July<span class="p">}</span>
     <span class="k">\label</span><span class="p">{</span>tab:temps<span class="p">}</span>
<span class="nt">\end{table}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/subtab.png" alt="Subtables" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h3 id="multi-rowcolumn-table">Multi-row/column table</h3>

<p>We may stack multiple rows (with <code class="language-plaintext highlighter-rouge">multirow</code>) or columns together. This can also be used to change the alignment of a specific cell. For example:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{tabular}</span><span class="p">{</span>|l|l|l|l|<span class="p">}</span><span class="k">\hline</span>
  <span class="k">\multirow</span><span class="p">{</span>10<span class="p">}{</span>*<span class="p">}{</span>numeric literals<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\multirow</span><span class="p">{</span>5<span class="p">}{</span>*<span class="p">}{</span>integers<span class="p">}</span> <span class="p">&amp;</span> in decimal <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">8743</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>3-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="k">\multirow</span><span class="p">{</span>2<span class="p">}{</span>*<span class="p">}{</span>in octal<span class="p">}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">0o7464</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">0O103</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>3-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="k">\multirow</span><span class="p">{</span>2<span class="p">}{</span>*<span class="p">}{</span>in hexadecimal<span class="p">}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">0x5A0FF</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">0xE0F2</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>2-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="k">\multirow</span><span class="p">{</span>5<span class="p">}{</span>*<span class="p">}{</span>fractionals<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\multirow</span><span class="p">{</span>5<span class="p">}{</span>*<span class="p">}{</span>in decimal<span class="p">}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">140.58</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">8.04e7</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">0.347E+12</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">5.47E-12</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">47e22</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>1-4<span class="p">}</span>
  <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>|l|<span class="p">}{</span><span class="k">\multirow</span><span class="p">{</span>3<span class="p">}{</span>*<span class="p">}{</span>char literals<span class="p">}}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">'H'</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>|l|<span class="p">}{}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">'\n'</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>          <span class="c">%% here</span>
  <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>|l|<span class="p">}{}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">'\x65'</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>1-4<span class="p">}</span>        <span class="c">%% here</span>
  <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>|l|<span class="p">}{</span><span class="k">\multirow</span><span class="p">{</span>2<span class="p">}{</span>*<span class="p">}{</span>string literals<span class="p">}}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">"bom dia"</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>4-4<span class="p">}</span>
  <span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>|l|<span class="p">}{}</span> <span class="p">&amp;</span> <span class="nb">\verb</span><span class="kp">|</span><span class="sx">"ouro preto\nmg"</span><span class="kp">|</span> <span class="k">\\</span> <span class="k">\cline</span><span class="p">{</span>1-4<span class="p">}</span>          <span class="c">%% here</span>
<span class="nt">\end{tabular}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/multi.png" alt="Multirow" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<p>Note that, by default, the vertical boarders in a <code class="language-plaintext highlighter-rouge">\multicolumn{}{}{}</code> is ignored, so must be specified if wanted.</p>

<h4 id="wrapping-text-in-a-column">Wrapping text in a column</h4>

<p>Specify a fixed-width column (<code class="language-plaintext highlighter-rouge">p{…}</code>) to get automatic line-breaks:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{tabular}</span><span class="p">{</span> | p<span class="p">{</span>0.7<span class="k">\linewidth</span><span class="p">}</span> | l | <span class="p">}</span> 
<span class="k">\hline</span>
<span class="k">\lipsum</span><span class="na">[1]</span> <span class="p">&amp;</span> Column 2 <span class="k">\\</span>
<span class="k">\hline</span>
<span class="nt">\end{tabular}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/long-row.jpg" alt="Long text in a column" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<h4 id="long-tables">Long tables</h4>

<p>For tables spanning pages, use <code class="language-plaintext highlighter-rouge">longtable</code> with customizable headers/footers:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{longtable}</span><span class="p">{</span>|c|c|c|<span class="p">}</span> 
<span class="k">\hline</span>
<span class="c">% Common header for all pages</span>
<span class="k">\textbf</span><span class="p">{</span>Column 1<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\textbf</span><span class="p">{</span>Column 2<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\textbf</span><span class="p">{</span>Column 3<span class="p">}</span> <span class="k">\\</span> 
<span class="k">\hline</span>
<span class="k">\endfirsthead</span> 
<span class="c">% Continued header for subsequent pages</span>
<span class="k">\hline</span>
<span class="k">\textbf</span><span class="p">{</span>Column 1<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\textbf</span><span class="p">{</span>Column 2<span class="p">}</span> <span class="p">&amp;</span> <span class="k">\textbf</span><span class="p">{</span>Column 3<span class="p">}</span> <span class="k">\\</span> 
<span class="k">\hline</span>
<span class="k">\endhead</span>
<span class="c">% Footer for intermediate pages</span>
<span class="k">\hline</span>
<span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>r<span class="p">}{</span><span class="k">\textit</span><span class="p">{</span>Continued on next page...<span class="p">}}</span> <span class="k">\\</span>
<span class="k">\endfoot</span>
<span class="c">% Footer for last page</span>
<span class="k">\hline</span>
<span class="k">\multicolumn</span><span class="p">{</span>3<span class="p">}{</span>c<span class="p">}{</span><span class="k">\textit</span><span class="p">{</span>End of table<span class="p">}}</span> <span class="k">\\</span> 
<span class="k">\endlastfoot</span>
<span class="c">% Table content</span>
1 <span class="p">&amp;</span> A <span class="p">&amp;</span> Alpha <span class="k">\\</span> 
2 <span class="p">&amp;</span> B <span class="p">&amp;</span> Beta <span class="k">\\</span>
...
<span class="nt">\end{longtable}</span>
</code></pre></div></div>

<hr />

<h2 id="equations">Equations</h2>

<p>Load <code class="language-plaintext highlighter-rouge">amsfonts,amsmath,amssymb</code> for all documents with math. For (un)numbered equation blocks, use the <code class="language-plaintext highlighter-rouge">align</code> environment without math mode. For example:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{align*}</span>
x<span class="p">&amp;</span>=y           <span class="p">&amp;</span>  w <span class="p">&amp;</span>=z              <span class="p">&amp;</span>  a<span class="p">&amp;</span>=b+c<span class="k">\\</span>
2x<span class="p">&amp;</span>=-y         <span class="p">&amp;</span>  3w<span class="p">&amp;</span>=<span class="k">\tfrac</span>12 z    <span class="p">&amp;</span>  a<span class="p">&amp;</span>=b<span class="k">\\</span>
-4 + 5x<span class="p">&amp;</span>=2+y   <span class="p">&amp;</span>  w+2<span class="p">&amp;</span>=-1+w          <span class="p">&amp;</span>  ab<span class="p">&amp;</span>=cb
<span class="nt">\end{align*}</span>
</code></pre></div></div>

\[\begin{aligned}
x &amp;= y &amp; w &amp;= z &amp; a &amp;= b + c \\
2x &amp;= -y &amp; 3w &amp;= \tfrac12 z &amp; a &amp;= b \\
-4 + 5x &amp;= 2 + y &amp; w + 2 &amp;= -1 + w &amp; ab &amp;= cb
\end{aligned}\]

<p>For a long, broken‐up equation:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{align*}</span>
F = <span class="p">&amp;</span> <span class="k">\{</span>F<span class="p">_{</span>x<span class="p">}</span><span class="k">\in</span> F<span class="p">_{</span>c<span class="p">}</span>: |S|&gt;|C|<span class="k">\\</span>
      <span class="p">&amp;</span><span class="k">\quad\cap</span> (<span class="k">\minPixels</span>&lt;|S|&lt;<span class="k">\maxPixels</span>)<span class="k">\\</span>
      <span class="p">&amp;</span><span class="k">\quad\cap</span>(|S<span class="p">_{</span><span class="k">\mathrm</span><span class="p">{</span>connected<span class="p">}}</span>|&gt;|S|-<span class="k">\epsilon</span>)<span class="k">\}</span>
<span class="nt">\end{align*}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/eq.png" alt="Long Equations" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<p>Note that any operators of the RHS should align with the right of the (in)equality sign. Also, do not put <code class="language-plaintext highlighter-rouge">\\</code> in blocks unless a new line is needed, otherwise extra space would be created:</p>

<p><img src="/assets/pdf/tex/badeq.png" alt="Bad long equations" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<p>Sometimes you may want to name a equation instead of numbering it, use <code class="language-plaintext highlighter-rouge">\tag{}</code> is this case:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{equation}</span>
Y = <span class="k">\alpha</span> + <span class="k">\beta</span> X<span class="p">^</span><span class="k">\top</span> + <span class="k">\varepsilon</span> <span class="k">\tag</span><span class="p">{</span>Baseline<span class="p">}</span> <span class="k">\label</span><span class="p">{</span>eq:baseline<span class="p">}</span>
<span class="nt">\end{equation}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/tageq.png" alt="Tagging equations" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<p><em>Note your math should end with punctuation if you want to embed it in a sentence.</em></p>

<hr />

<h2 id="theorems-and-definitions">Theorems and Definitions</h2>

<p>Use <code class="language-plaintext highlighter-rouge">amsthm</code> for theorems, definitions, remarks, etc. Define environments in the preamble:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\newtheorem</span><span class="p">{</span>theorem<span class="p">}{</span>Theorem<span class="p">}</span>
<span class="k">\theoremstyle</span><span class="p">{</span>remark<span class="p">}</span>
<span class="k">\newtheorem*</span><span class="p">{</span>remark<span class="p">}{</span>Remark<span class="p">}</span>
<span class="k">\theoremstyle</span><span class="p">{</span>definition<span class="p">}</span>
<span class="k">\newtheorem</span><span class="p">{</span>definition<span class="p">}{</span>Definition<span class="p">}</span>
</code></pre></div></div>

<p>There are three styles of blocks available, <code class="language-plaintext highlighter-rouge">plain</code>, <code class="language-plaintext highlighter-rouge">definition</code>, and <code class="language-plaintext highlighter-rouge">remark</code>. To number according to sections, chapters or theorem (for corollaries), add <code class="language-plaintext highlighter-rouge">[anchor]</code> at the end of the definition in the preamble. For example, adding <code class="language-plaintext highlighter-rouge">\newtheorem{corollary}{Corollary}[theorem]</code> produces <code class="language-plaintext highlighter-rouge">Corollary 1.1</code>.</p>

<p>For a box wrapping the block, use <code class="language-plaintext highlighter-rouge">mdframed</code>. Wrap the block by the <code class="language-plaintext highlighter-rouge">mdframed</code> environment for a local change. To wrap all the blocks, put the following in the preamble:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\surroundwithmdframed</span><span class="p">{</span>theorem<span class="p">}</span>
<span class="k">\surroundwithmdframed</span><span class="p">{</span>definition<span class="p">}</span>
<span class="k">\surroundwithmdframed</span><span class="p">{</span>remark<span class="p">}</span>
</code></pre></div></div>

<p>Then the following code</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{theorem}</span>[Differentiability Implies Countintuity] <span class="k">\label</span><span class="p">{</span>thm:diff<span class="p">}</span>
Let <span class="p">\(</span><span class="nb">f</span><span class="p">\)</span> be a function whose derivative exists in every point, then <span class="p">\(</span><span class="nb">f</span><span class="p">\)</span> is a continuous function.
<span class="nt">\end{theorem}</span>

<span class="nt">\begin{corollary}</span>
<span class="p">\(</span><span class="nb">f</span><span class="nv">\left</span><span class="o">(</span><span class="nb">x,y</span><span class="nv">\right</span><span class="o">)=</span><span class="nb">x</span><span class="o">+</span><span class="nb">y</span><span class="p">\)</span> is continuous and differentiable everywhere. 
<span class="nt">\end{corollary}</span>

<span class="nt">\begin{proof}</span>[Proof of <span class="k">\autoref</span><span class="p">{</span>thm:diff<span class="p">}</span>]
To prove that <span class="p">\(</span><span class="nb"> f </span><span class="p">\)</span> is continuous at any arbitrary point <span class="p">\(</span><span class="nb"> a </span><span class="p">\)</span> in its domain, we need to show that:
<span class="p">\[</span><span class="nb">
</span><span class="nv">\lim</span><span class="p">_{</span><span class="nb">x </span><span class="nv">\to</span><span class="nb"> a</span><span class="p">}</span><span class="nb"> f</span><span class="o">(</span><span class="nb">x</span><span class="o">)</span><span class="nb"> </span><span class="o">=</span><span class="nb"> f</span><span class="o">(</span><span class="nb">a</span><span class="o">)</span><span class="nb">.
</span><span class="p">\]</span>

...
<span class="nt">\end{proof}</span>

<span class="nt">\begin{theorem}</span>
<span class="k">\mintinline</span><span class="p">{</span>latex<span class="p">}{</span>plain<span class="p">}</span> has boldface title, italicized body. 
Commonly used in theorems, lemmas, corollaries, propositions and conjectures.
<span class="nt">\end{theorem}</span>

<span class="nt">\begin{definition}</span>
<span class="k">\mintinline</span><span class="p">{</span>latex<span class="p">}{</span>definition<span class="p">}</span> has boldface title, Roman body. 
Commonly used in definitions, conditions, problems and examples.
<span class="nt">\end{definition}</span>

<span class="nt">\begin{remark}</span>
<span class="k">\mintinline</span><span class="p">{</span>latex<span class="p">}{</span>remark<span class="p">}</span> has italicized title, Roman body. 
Commonly used in remarks, notes, annotations, claims, cases, acknowledgments and conclusions.
<span class="nt">\end{remark}</span>

<span class="nt">\begin{mdframed}</span>
<span class="nt">\begin{theorem}</span>[Pythagorean theorem]
<span class="k">\label</span><span class="p">{</span>pythagorean<span class="p">}</span>
This is a theorem about right triangles and can be summarised in the next 
equation 
<span class="p">\[</span><span class="nb"> x</span><span class="p">^</span><span class="m">2</span><span class="nb"> </span><span class="o">+</span><span class="nb"> y</span><span class="p">^</span><span class="m">2</span><span class="nb"> </span><span class="o">=</span><span class="nb"> z</span><span class="p">^</span><span class="m">2</span><span class="nb">. </span><span class="p">\]</span>
<span class="nt">\end{theorem}</span>
<span class="nt">\end{mdframed}</span>
</code></pre></div></div>
<p>would produce
<img src="/assets/pdf/tex/amsthm.png" alt="Theorems" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<hr />

<h2 id="code">Code</h2>

<ul>
  <li>
    <p><strong>Inline code:</strong> use backticks: <code class="language-plaintext highlighter-rouge">\verb|…|</code> or <code class="language-plaintext highlighter-rouge">\mintinline{python}{…}</code></p>
  </li>
  <li>
    <p><strong>Display code (no highlighting):</strong></p>

    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{verbatim}</span>
from sklearn.neural<span class="p">_</span>network import MLPClassifier
<span class="nt">\end{verbatim}</span>
</code></pre></div>    </div>
    <p>gives</p>
    <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.neural_network import MLPClassifier
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>With <code class="language-plaintext highlighter-rouge">minted</code> (syntax-highlighted):</strong></p>

    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{minted}</span><span class="p">{</span>latex<span class="p">}</span>
from sklearn.neural<span class="p">_</span>network import MLPClassifier
<span class="nt">\end{minted}</span>
</code></pre></div>    </div>
    <p>gives</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Note that inline code snippets are non-breakable. To caption a <code class="language-plaintext highlighter-rouge">minted</code> block, wrap it in a <code class="language-plaintext highlighter-rouge">listing</code> float just like figures/tables.</p>

<hr />

<h2 id="algorithms">Algorithms</h2>

<p>Use the <code class="language-plaintext highlighter-rouge">algorithm</code> + <code class="language-plaintext highlighter-rouge">algorithmic</code> packages:</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{algorithm}</span>[htbp]
<span class="k">\caption</span><span class="p">{</span>Gibbs Sampler<span class="p">}</span>
<span class="k">\label</span><span class="p">{</span>alg:gibbs<span class="p">}</span>
<span class="nt">\begin{algorithmic}</span>[1]
  <span class="k">\STATE</span> Initialize <span class="p">$</span><span class="nb">X</span><span class="p">_</span><span class="m">1</span><span class="nb">,</span><span class="nv">\dots</span><span class="nb">,X</span><span class="p">_</span><span class="nb">n</span><span class="p">$</span>.
  <span class="k">\FOR</span><span class="p">{$</span><span class="nb">t</span><span class="o">=</span><span class="m">1</span><span class="p">$</span> to <span class="p">$</span><span class="nb">T</span><span class="p">$}</span>
    <span class="k">\FOR</span><span class="p">{$</span><span class="nb">i</span><span class="o">=</span><span class="m">1</span><span class="p">$</span> to <span class="p">$</span><span class="nb">n</span><span class="p">$}</span>
      <span class="k">\STATE</span> Sample <span class="p">$</span><span class="nb">X</span><span class="p">_</span><span class="nb">i</span><span class="p">^{</span><span class="o">(</span><span class="nb">t</span><span class="o">)</span><span class="p">}</span><span class="nv">\sim</span><span class="nb"> P</span><span class="o">(</span><span class="nb">X</span><span class="p">_</span><span class="nb">i</span><span class="nv">\mid</span><span class="nb"> X</span><span class="p">_{</span><span class="o">-</span><span class="nb">i</span><span class="p">}</span><span class="o">)</span><span class="p">$</span>.
    <span class="k">\ENDFOR</span>
  <span class="k">\ENDFOR</span>
  <span class="k">\STATE</span> Return <span class="p">$</span><span class="nv">\{</span><span class="nb">X</span><span class="p">^{</span><span class="o">(</span><span class="nb">t</span><span class="o">)</span><span class="p">}</span><span class="nv">\}</span><span class="p">_{</span><span class="nb">t</span><span class="o">=</span><span class="m">1</span><span class="p">}^</span><span class="nb">T</span><span class="p">$</span>.
<span class="nt">\end{algorithmic}</span>
<span class="nt">\end{algorithm}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/algo.jpg" alt="Algorithms" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>

<hr />

<h2 id="layout">Layout</h2>

<ul>
  <li><strong>Quotation marks:</strong> use  ``  and <code class="language-plaintext highlighter-rouge">''</code>.</li>
  <li>
    <p><strong>Margins:</strong> To adjust margins and paper size, use the following code in the preamble:</p>

    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="p">{</span>geometry<span class="p">}</span>
<span class="k">\geometry</span><span class="p">{</span>a4paper, margin=1in<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Fonts:</strong> It is recommended to use pdfTeX. Use <code class="language-plaintext highlighter-rouge">newtxtext</code>/<code class="language-plaintext highlighter-rouge">newtxmath</code> for Times, <code class="language-plaintext highlighter-rouge">newpxtext</code>/<code class="language-plaintext highlighter-rouge">newpxmath</code> for Palatino.</li>
  <li><strong>Font size:</strong> Document-level: <code class="language-plaintext highlighter-rouge">\documentclass[12pt]{article}</code>; local: <code class="language-plaintext highlighter-rouge">{\small …}</code> etc.:
<img src="/assets/pdf/tex/size.png" alt="Font size" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></li>
  <li>
    <p><strong>Hyperlinks:</strong> To use hyperlinks, use <code class="language-plaintext highlighter-rouge">hyperref</code> for general links and <code class="language-plaintext highlighter-rouge">url</code> for URLs. To customize colors, do something like:</p>

    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="p">{</span>hyperref<span class="p">}</span>
<span class="k">\usepackage</span><span class="na">[dvipsnames]</span><span class="p">{</span>xcolor<span class="p">}</span>
<span class="k">\usepackage</span><span class="p">{</span>hyperref<span class="p">}</span>
<span class="k">\definecolor</span><span class="p">{</span>DarkNavy<span class="p">}{</span>rgb<span class="p">}{</span>0.0, 0.0, 0.5<span class="p">}</span>
<span class="k">\definecolor</span><span class="p">{</span>DeepMaroon<span class="p">}{</span>rgb<span class="p">}{</span>0.5, 0.0, 0.0<span class="p">}</span>
<span class="k">\hypersetup</span><span class="p">{</span>
  colorlinks   = true,
  linkcolor    = DarkNavy,     <span class="c">% for \ref and internal links</span>
  citecolor    = DeepMaroon,   <span class="c">% for \cite</span>
  urlcolor     = DarkNavy,     <span class="c">% for external URLs</span>
  filecolor    = OliveGreen    <span class="c">% for local file links</span>
<span class="p">}</span>
</code></pre></div>    </div>

    <p>Use <code class="language-plaintext highlighter-rouge">\autoref{…}</code> and <code class="language-plaintext highlighter-rouge">\nameref{…}</code> for links. For example, referring back to the images in <a href="#figures">Figures</a>, <code class="language-plaintext highlighter-rouge">\autoref{fig:1-image}</code> and <code class="language-plaintext highlighter-rouge">\autoref{fig:2-image-a}</code> would produce <code class="language-plaintext highlighter-rouge">Figure 1 and Figure 2a</code>. Use meaningful labels with prefix (for example, the label for <code class="language-plaintext highlighter-rouge">Algorithm 1</code> is <code class="language-plaintext highlighter-rouge">\label{alg:gibbs}</code>) so that it would be easy to recall them. However, for some floats, such as algorithm boxes, the name for references might not be predefined. In these cases, use the command <code class="language-plaintext highlighter-rouge">\newcommand{\algorithmautorefname}{Algorithm}</code> to define them. If it is already defined but needs to be changed (say, capitalizing), use <code class="language-plaintext highlighter-rouge">\renewcommand{\algorithmautorefname}{Algorithm}</code>. To refer directly to the name of the section or the caption of the float, use, for example, <code class="language-plaintext highlighter-rouge">\nameref{sec:figure}</code> and <code class="language-plaintext highlighter-rouge">\nameref{alg:gibbs}</code> for <code class="language-plaintext highlighter-rouge">Figures and Gibbs Sampler</code>. This works for math similarly, <code class="language-plaintext highlighter-rouge">\autoref{eq:baseline}</code> produces <code class="language-plaintext highlighter-rouge">Equation Baseline</code>. It also works for theorems.</p>
  </li>
  <li><strong>Line spacing:</strong> Use <code class="language-plaintext highlighter-rouge">\onehalfspacing</code> (via <code class="language-plaintext highlighter-rouge">setspace</code> package). Note that this does not change spaces in other environments, such as captions. However, note that this means that the original space between lines are multiplied by 1.5. This is different from Microsoft Word or Apple Pages, which set the space between lines as 1.5 times the vertical space of texts.</li>
  <li><strong>Paragraph spacing:</strong> Load <code class="language-plaintext highlighter-rouge">parskip</code> to disable indentations and use spacing to distinguish paragraphs.</li>
  <li><strong>Indentations:</strong> Disable locally with <code class="language-plaintext highlighter-rouge">\noindent</code>, or load <code class="language-plaintext highlighter-rouge">indentfirst</code> to indent first paragraphs.</li>
  <li><strong>Forced spaces:</strong> In some cases, spaces are ignored after a command, use <code class="language-plaintext highlighter-rouge">~</code> to force it, e.g. <code class="language-plaintext highlighter-rouge">\LaTeX~document</code>.</li>
  <li><strong>Vertical space:</strong> Use <code class="language-plaintext highlighter-rouge">\vspace{…}</code> but it automatically halts at the beginning/end of a page, use <code class="language-plaintext highlighter-rouge">\vspace*{…}</code> to force it.</li>
  <li>
    <p><strong>Accented letters:</strong> To use accented letters directly (copying and pasting), load</p>

    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="na">[utf8]</span><span class="p">{</span>inputenc<span class="p">}</span> <span class="c">% usually not needed (loaded by default)</span>
<span class="k">\usepackage</span><span class="na">[T1]</span><span class="p">{</span>fontenc<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Landscape pages:</strong></p>

    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\usepackage</span><span class="p">{</span>pdflscape<span class="p">}</span>
…
<span class="k">\clearpage</span>
<span class="nt">\begin{landscape}</span>
  … 
<span class="nt">\end{landscape}</span>
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="tikz">TikZ</h2>

<p>Using TikZ for illustration makes the content clear and consistent. Use PowerPoint/Keynotes/Google Slides to make your life easier.</p>

<div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{figure}</span>[htbp]
    <span class="k">\centering</span>
    <span class="nt">\begin{tikzpicture}</span>[
        tree/.style=<span class="p">{</span>rectangle, draw, fill=blue!10, text width=2cm, text centered, rounded corners<span class="p">}</span>,
        bootstrap/.style=<span class="p">{</span>ellipse, draw, fill=green!10, minimum height=1cm, text centered<span class="p">}</span>,
        box/.style=<span class="p">{</span>rectangle, draw, minimum width=1.5cm, minimum height=1.2cm, fill=red!10<span class="p">}</span>,
        arrow/.style=<span class="p">{</span>thick,-&gt;,&gt;=stealth<span class="p">}</span>
    ]

    <span class="c">% Original Dataset</span>
    <span class="k">\node</span><span class="na">[box]</span> (data) <span class="p">{</span>Original Dataset<span class="p">}</span>;

    <span class="c">% Bootstrap Samples</span>
    <span class="k">\node</span><span class="na">[bootstrap, below left=1.5cm and 2cm of data]</span> (sample1) <span class="p">{</span>Bootstrap Sample 1<span class="p">}</span>;
    <span class="k">\node</span><span class="na">[bootstrap, below=1.5cm of data]</span> (sample2) <span class="p">{</span>Bootstrap Sample 2<span class="p">}</span>;
    <span class="k">\node</span><span class="na">[bootstrap, below right=1.5cm and 2cm of data]</span> (sample3) <span class="p">{</span>Bootstrap Sample 3<span class="p">}</span>;

    <span class="c">% Decision Trees</span>
    <span class="k">\node</span><span class="na">[tree, below=2cm of sample1]</span> (tree1) <span class="p">{</span>Tree 1<span class="p">}</span>;
    <span class="k">\node</span><span class="na">[tree, below=2cm of sample2]</span> (tree2) <span class="p">{</span>Tree 2<span class="p">}</span>;
    <span class="k">\node</span><span class="na">[tree, below=2cm of sample3]</span> (tree3) <span class="p">{</span>Tree 3<span class="p">}</span>;

    <span class="c">% Aggregation</span>
    <span class="k">\node</span><span class="na">[box, below=1cm of tree2]</span> (aggregate) <span class="p">{</span>Aggregate Prediction<span class="p">}</span>;

    <span class="c">% Connections</span>
    <span class="k">\draw</span><span class="na">[arrow]</span> (data) -- (sample1);
    <span class="k">\draw</span><span class="na">[arrow]</span> (data) -- (sample2);
    <span class="k">\draw</span><span class="na">[arrow]</span> (data) -- (sample3);

    <span class="k">\draw</span><span class="na">[arrow]</span> (sample1) -- (tree1);
    <span class="k">\draw</span><span class="na">[arrow]</span> (sample2) -- (tree2);
    <span class="k">\draw</span><span class="na">[arrow]</span> (sample3) -- (tree3);

    <span class="k">\draw</span><span class="na">[arrow]</span> (tree1) -- (aggregate);
    <span class="k">\draw</span><span class="na">[arrow]</span> (tree2) -- (aggregate);
    <span class="k">\draw</span><span class="na">[arrow]</span> (tree3) -- (aggregate);

    <span class="nt">\end{tikzpicture}</span>
    <span class="k">\caption</span><span class="p">{</span>Illustration of Bagged Decision Trees. 
    Multiple bootstrap samples are drawn from the original dataset, 
    and individual decision trees are trained on each sample. 
    The final prediction is obtained by aggregating the outputs of all trees.<span class="p">}</span>
    <span class="k">\label</span><span class="p">{</span>fig:bagged<span class="p">_</span>decision<span class="p">_</span>trees<span class="p">}</span>
<span class="nt">\end{figure}</span>
</code></pre></div></div>

<p><img src="/assets/pdf/tex/tikz.png" alt="TikZ" style="max-width:100%; height:auto; display:block; margin:1rem auto;" /></p>]]></content><author><name></name></author><category term="LaTeX" /><category term="Typesetting" /><summary type="html"><![CDATA[]]></summary></entry></feed>